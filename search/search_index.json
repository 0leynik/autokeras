{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Auto-Keras is an open source software library for automated machine learning (AutoML). It is developed by DATA Lab at Texas A&M University and community contributors. The ultimate goal of AutoML is to provide easily accessible deep learning tools to domain experts with limited data science or machine learning background. Auto-Keras provides functions to automatically search for architecture and hyperparameters of deep learning models. Installation To install the package, please use the pip installation as follows: pip install autokeras Note: currently, Auto-Keras is only compatible with: Python 3.6 . Example Here is a short example of using the package. import autokeras as ak clf = ak.ImageClassifier() clf.fit(x_train, y_train) results = clf.predict(x_test) Community You can use Gitter to communicate with people who are also interested in Auto-Keras. You can follow us on Twitter. Follow @autokeras Cite this work Auto-Keras: Efficient Neural Architecture Search with Network Morphism. Haifeng Jin, Qingquan Song, and Xia Hu. arXiv:1806.10282 . Biblatex entry: @online{jin2018efficient, author = {Haifeng Jin and Qingquan Song and Xia Hu}, title = {Auto-Keras: Efficient Neural Architecture Search with Network Morphism}, date = {2018-06-27}, year = {2018}, eprintclass = {cs.LG}, eprinttype = {arXiv}, eprint = {cs.LG/1806.10282}, } Support Auto-Keras We accept donations on Open Collective . Thank every backer for supporting us! DISCLAIMER Please note that this is a pre-release version of the Auto-Keras which is still undergoing final testing before its official release. The website, its software and all content found on it are provided on an \u201cas is\u201d and \u201cas available\u201d basis. Auto-Keras does not give any warranties, whether express or implied, as to the suitability or usability of the website, its software or any of its content. Auto-Keras will not be liable for any loss, whether such loss is direct, indirect, special or consequential, suffered by any party as a result of their use of the libraries or content. Any usage of the libraries is done at the user\u2019s own risk and the user will be solely responsible for any damage to any computer system or loss of data that results from such activities. Should you encounter any bugs, glitches, lack of functionality or other problems on the website, please let us know immediately so we can rectify these accordingly. Your help in this regard is greatly appreciated. Acknowledgements The authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&M College of Engineering, and Texas A&M.","title":"Home"},{"location":"#installation","text":"To install the package, please use the pip installation as follows: pip install autokeras Note: currently, Auto-Keras is only compatible with: Python 3.6 .","title":"Installation"},{"location":"#example","text":"Here is a short example of using the package. import autokeras as ak clf = ak.ImageClassifier() clf.fit(x_train, y_train) results = clf.predict(x_test)","title":"Example"},{"location":"#community","text":"You can use Gitter to communicate with people who are also interested in Auto-Keras. You can follow us on Twitter. Follow @autokeras","title":"Community"},{"location":"#cite-this-work","text":"Auto-Keras: Efficient Neural Architecture Search with Network Morphism. Haifeng Jin, Qingquan Song, and Xia Hu. arXiv:1806.10282 . Biblatex entry: @online{jin2018efficient, author = {Haifeng Jin and Qingquan Song and Xia Hu}, title = {Auto-Keras: Efficient Neural Architecture Search with Network Morphism}, date = {2018-06-27}, year = {2018}, eprintclass = {cs.LG}, eprinttype = {arXiv}, eprint = {cs.LG/1806.10282}, }","title":"Cite this work"},{"location":"#support-auto-keras","text":"We accept donations on Open Collective . Thank every backer for supporting us!","title":"Support Auto-Keras"},{"location":"#disclaimer","text":"Please note that this is a pre-release version of the Auto-Keras which is still undergoing final testing before its official release. The website, its software and all content found on it are provided on an \u201cas is\u201d and \u201cas available\u201d basis. Auto-Keras does not give any warranties, whether express or implied, as to the suitability or usability of the website, its software or any of its content. Auto-Keras will not be liable for any loss, whether such loss is direct, indirect, special or consequential, suffered by any party as a result of their use of the libraries or content. Any usage of the libraries is done at the user\u2019s own risk and the user will be solely responsible for any damage to any computer system or loss of data that results from such activities. Should you encounter any bugs, glitches, lack of functionality or other problems on the website, please let us know immediately so we can rectify these accordingly. Your help in this regard is greatly appreciated.","title":"DISCLAIMER"},{"location":"#acknowledgements","text":"The authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&M College of Engineering, and Texas A&M.","title":"Acknowledgements"},{"location":"about/","text":"About This package is developed by DATA LAB at Texas A&M University and community contributors.","title":"About"},{"location":"about/#about","text":"This package is developed by DATA LAB at Texas A&M University and community contributors.","title":"About"},{"location":"docker/","text":"Auto-Keras Docker Download Auto-Keras Docker image The following command download Auto-Keras docker image to your machine. docker pull garawalid/autokeras Start Auto-Keras Docker container docker run -it --shm-size 2G garawalid/autokeras /bin/bash In case you need more memory to run the container, change the value of shm-size . ( Docker run reference ) Run application : To run a local script file.py using Auto-Keras within the container, mount the host directory -v hostDir:/app . docker run -it -v hostDir:/app --shm-size 2G garawalid/autokeras python file.py Example : Let's download the mnist example and run it within the container. Download the example : wget https://raw.githubusercontent.com/jhfjhfj1/autokeras/master/examples/a_simple_example/mnist.py Run the mnist example : docker run -it -v \"$(pwd)\":/app --shm-size 2G garawalid/autokeras python mnist.py","title":"Docker"},{"location":"docker/#auto-keras-docker","text":"","title":"Auto-Keras Docker"},{"location":"docker/#download-auto-keras-docker-image","text":"The following command download Auto-Keras docker image to your machine. docker pull garawalid/autokeras","title":"Download Auto-Keras Docker image"},{"location":"docker/#start-auto-keras-docker-container","text":"docker run -it --shm-size 2G garawalid/autokeras /bin/bash In case you need more memory to run the container, change the value of shm-size . ( Docker run reference )","title":"Start Auto-Keras Docker container"},{"location":"docker/#run-application","text":"To run a local script file.py using Auto-Keras within the container, mount the host directory -v hostDir:/app . docker run -it -v hostDir:/app --shm-size 2G garawalid/autokeras python file.py","title":"Run application :"},{"location":"docker/#example","text":"Let's download the mnist example and run it within the container. Download the example : wget https://raw.githubusercontent.com/jhfjhfj1/autokeras/master/examples/a_simple_example/mnist.py Run the mnist example : docker run -it -v \"$(pwd)\":/app --shm-size 2G garawalid/autokeras python mnist.py","title":"Example :"},{"location":"start/","text":"Getting Started Installation The installation of Auto-Keras is the same as other python packages. Note: currently, Auto-Keras is only compatible with: Python 3.6 . Latest Stable Version ( pip installation): You can run the following pip installation command in your terminal to install the latest stable version. pip install autokeras Bleeding Edge Version (manual installation): If you want to install the latest development version. You need to download the code from the GitHub repo and run the following commands in the project directory. pip install -r requirements.txt python setup.py install A Simple Example We show an example of image classification on the MNIST dataset, which is a famous benchmark image dataset for hand-written digits classification. Auto-Keras supports different types of data inputs. Data with numpy array (.npy) format. [source] If the images and the labels are already formatted into numpy arrays, you can from keras.datasets import mnist from autokeras.image.image_supervised import ImageClassifier if __name__ == '__main__': (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(x_train.shape + (1,)) x_test = x_test.reshape(x_test.shape + (1,)) clf = ImageClassifier(verbose=True) clf.fit(x_train, y_train, time_limit=12 * 60 * 60) clf.final_fit(x_train, y_train, x_test, y_test, retrain=True) y = clf.evaluate(x_test, y_test) print(y) In the example above, the images and the labels are already formatted into numpy arrays. What if your data are raw image files ( e.g. .jpg, .png, .bmp)? [source] You can use our load_image_dataset function to load the images and their labels as follows. from autokeras.image.image_supervised import load_image_dataset x_train, y_train = load_image_dataset(csv_file_path=\"train/label.csv\", images_path=\"train\") print(x_train.shape) print(y_train.shape) x_test, y_test = load_image_dataset(csv_file_path=\"test/label.csv\", images_path=\"test\") print(x_test.shape) print(y_test.shape) The argument csv_file_path is the path to the CSV file containing the image file names and their corresponding labels. Both csv files and the raw image datasets could be downloaded from link . Here is an example of the csv file. File Name,Label 00000.jpg,5 00001.jpg,0 00002.jpg,4 00003.jpg,1 00004.jpg,9 00005.jpg,2 00006.jpg,1 ... The second argument images_path is the path to the directory containing all the images with those file names listed in the CSV file. The returned values x_train and y_train are the numpy arrays, which can be directly feed into the fit function of ImageClassifier . This CSV file for train or test can be created from folders containing images of a specific class (meaning label): train \u2514\u2500\u2500\u2500class_1 \u2502 \u2502 class_1_image_1.png \u2502 \u2502 class_1_image_2.png | | ... \u2514\u2500\u2500\u2500class_2 \u2502 class_2_image_1.png \u2502 class_2_image_2.png | ... The code below shows an example of how to create the CSV: train_dir = 'train' # Path to the train directory class_dirs = [i for i in os.listdir(path=train_dir) if os.path.isdir(os.path.join(train_dir, i))] with open('train/label.csv', 'w') as train_csv: fieldnames = ['File Name', 'Label'] writer = csv.DictWriter(train_csv, fieldnames=fieldnames) writer.writeheader() label = 0 for current_class in class_dirs: for image in os.listdir(os.path.join(train_dir, current_class)): writer.writerow({'File Name': str(image), 'Label':label}) label += 1 train_csv.close() Portable Models How to export keras models? clf.load_searcher().load_best_model().produce_keras_model().save('my_model.h5') This uses the keras function model.save() to export a single HDF5 file containing the architecture of the model, the weights of the model, the training configuration, and the state of the optimizer. See https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model Note: This is being built into AutoKeras as ImageClassifier().export_keras_model() How to export Portable model? [source] from autokeras import ImageClassifier clf = ImageClassifier(verbose=True, augment=False) clf.export_autokeras_model(model_file_name) The model will be stored into the path model_file_name . How to load exported Portable model? [source] from autokeras.utils import pickle_from_file model = pickle_from_file(model_file_name) results = model.evaluate(x_test, y_test) print(results) The model will be loaded from the path model_file_name and then you can use the functions listed in PortableImageSupervised . Model Visualizations How to visualize keras models? This is not specific to AutoKeras, however, the following will generate a .PNG visualization of the best model found by AutoKeras: from keras.models import load_model model = load_model('my_model.h5') #See 'How to export keras models?' to generate this file before loading it. from keras.utils import plot_model plot_model(model, to_file='my_model.png') How to visualize the best selected architecture? [source] While trying to create a model, let's say an Image classifier on MNIST, there is a facility for the user to visualize a .PDF depiction of the best architecture that was chosen by autokeras, after model training is complete. Prerequisites : 1) graphviz must be installed in your system. Refer Installation Guide 2) Additionally, also install \"graphviz\" python package using pip / conda pip: pip install graphviz conda : conda install -c conda-forge python-graphviz If the above installations are complete, proceed with the following steps : Step 1 : Specify a path before starting your model training clf = ImageClassifier(path=\"~/automodels/\",verbose=True, augment=False) # Give a custom path of your choice clf.fit(x_train, y_train, time_limit=30 * 60) clf.final_fit(x_train, y_train, x_test, y_test, retrain=True) Step 2 : After the model training is complete, run examples/visualize.py , whilst passing the same path as parameter if __name__ == '__main__': visualize('~/automodels/') Net Modules MlpModule tutorial. [source] MlpGenerator in net_module.py is a child class of Networkmodule . It can generates neural architecture with MLP modules Normally, there's two place to call the MlpGenerator, one is call MlpGenerator.fit while the other is MlpGenerator.final_fit . For example, in a image classification class ImageClassifier , one can initialize the cnn module as: mlpModule = MlpModule(loss, metric, searcher_args, path, verbose) Where: loss and metric determines by the type of training model(classification or regression or others) search_args can be referred in search.py path is the path to store the whole searching process and generated model. verbose is a boolean. Setting it to true prints to stdout. Then, for the searching part, one can call: mlpModule.fit(n_output_node, input_shape, train_data, test_data, time_limit=24 * 60 * 60) where: n_output_node: A integer value represent the number of output node in the final layer. input_shape: A tuple to express the shape of every train entry. For example, MNIST dataset would be (28,28,1). train_data: A PyTorch DataLoader instance representing the training data. test_data: A PyTorch DataLoader instance representing the testing data. * time_limit: A integer value represents the time limit on searching for models. And for final testing(testing the best searched model), one can call: mlpModule.final_fit(train_data, test_data, trainer_args=None, retrain=False) where: train_data: A DataLoader instance representing the training data. test_data: A DataLoader instance representing the testing data. trainer_args: A dictionary containing the parameters of the ModelTrainer constructor. retrain: A boolean of whether reinitialize the weights of the model. CnnModule tutorial. [source] CnnGenerator in net_module.py is a child class of Networkmodule . It can generates neural architecture with basic cnn modules and the ResNet module. Normally, there's two place to call the CnnGenerator, one is call CnnGenerator.fit while the other is CnnGenerator.final_fit . For example, in a image classification class ImageClassifier , one can initialize the cnn module as: from autokeras import CnnModule from autokeras.nn.loss_function import classification_loss from autokeras.nn.metric import Accuracy TEST_FOLDER = \"test\" cnnModule = CnnModule(loss=classification_loss, metric=Accuracy, searcher_args={}, path=TEST_FOLDER, verbose=False) Where: loss and metric determines by the type of training model(classification or regression or others) search_args can be referred in search.py path is the path to store the whole searching process and generated model. verbose is a boolean. Setting it to true prints to stdout. Then, for the searching part, one can call: cnnModule.fit(n_output_node, input_shape, train_data, test_data, time_limit=24 * 60 * 60) where: n_output_node: A integer value represent the number of output node in the final layer. input_shape: A tuple to express the shape of every train entry. For example, MNIST dataset would be (28,28,1). train_data: A PyTorch DataLoader instance representing the training data. test_data: A PyTorch DataLoader instance representing the testing data. * time_limit: A integer value represents the time limit on searching for models. And for final testing(testing the best searched model), one can call: cnnModule.final_fit(train_data, test_data, trainer_args=None, retrain=False) where: train_data: A DataLoader instance representing the training data. test_data: A DataLoader instance representing the testing data. trainer_args: A dictionary containing the parameters of the ModelTrainer constructor. retrain: A boolean of whether reinitialize the weights of the model. Task Modules Automated text classifier tutorial. [source] Class TextClassifier and TextRegressor are designed for automated generate best performance cnn neural architecture for a given text dataset. clf = TextClassifier(verbose=True) clf.fit(x=x_train, y=y_train, batch_size=10, time_limit=12 * 60 * 60) x_train: string format text data y_train: int format text label After searching the best model, one can call clf.final_fit to test the best model found in searching. Notes: Preprocessing of the text data: Class TextClassifier and TextRegressor contains a pre-process of the text data. Which means the input data should be in string format. The default pre-process model uses the glove6B model from Stanford NLP. * To change the default setting of the pre-process model, one need to change the corresponding variable: EMBEDDING_DIM , PRE_TRAIN_FILE_LINK , PRE_TRAIN_FILE_LINK , PRE_TRAIN_FILE_NAME in constant.py . Automated tabular classifier tutorial. [source] Class TabularClassifier and TabularRegressor are designed for automated generate best performance shallow/deep architecture for a given tabular dataset. (Currently, theis module only supports lightgbm classifier and regressor.) clf = TabularClassifier(verbose=True) clf.fit(x_train, y_train, time_limit=12 * 60 * 60, data_info=datainfo) x_train: string format text data y_train: int format text label data_info: a numpy.array describing the feature types (time, numerical or categorical) of each column in x_train. Notes: Preprocessing of the tabular data: Class [TabularPreprocessor] involves several automated feature preprocessing and engineering operation for tabular data . The input data should be in numpy array format for the class TabularClassifier and TabularRegressor . Pretrained Models Object detection tutorial. [source] by Wuyang Chen from Dr. Atlas Wang's group at CSE Department, Texas A&M. class_id_mapping = {0 : \"Business\", 1 : \"Sci/Tech\", 2 : \"Sports\", 3 : \"World\"} ObjectDetector in object_detector.py is a child class of Pretrained . Currently it can load a pretrained SSD model ( Liu, Wei, et al. \"Ssd: Single shot multibox detector.\" European conference on computer vision. Springer, Cham, 2016. ) and find object(s) in a given image. Let's first import the ObjectDetector and create a detection model ( detector ) with from autokeras.pretrained.object_detector import ObjectDetector detector = ObjectDetector() Note: the ObjectDetector class can automatically detect the existance of available cuda device(s), and use the device if exists. Second, you will want to load the pretrained weights for your model: detector.load() This line will automatically download and load the weights into detector . Finally you can make predictions against an image: results = detector.predict(\"/path/to/images/000001.jpg\", output_file_path=\"/path/to/images/\") Function detector.predict() requires the path to the image. If the output_file_path is not given, the detector will just return the numerical results as a list of dictionaries. Each dictionary is like {\"left\": int, \"top\": int, \"width\": int, \"height\": int: \"category\": str, \"confidence\": float}, where left and top is the (left, top) coordinates of the bounding box of the object and width and height are width and height of the box. category is a string representing the class the object belongs to, and the confidence can be regarded as the probability that the model believes its prediction is correct. If the output_file_path is given, then the results mentioned above will be plotted and saved in a new image file with suffix \"_prediction\" into the given output_file_path . If you run the example/object_detection/object_detection_example.py, you will get result [{'category': 'person', 'width': 331, 'height': 500, 'left': 17, 'confidence': 0.9741123914718628, 'top': 0}] Sentiment Analysis tutorial. [source] The sentiment analysis module provides an interface to find the sentiment of any text input. The pretrained model is obtained by training Google AI\u2019s BERT model on IMDb dataset . Let\u2019s import the SentimentAnalysis module from sentiment_analysis.py . It is derived from the super class Pretrained . from autokeras.pretrained.text_sentiment import SentimentAnalysis sentiment_cls = SentimentAnalysis() During initialization of SentimentAnalysis , the pretrained model is loaded into memory i.e. CPU\u2019s or GPU\u2019s, if available. Now, you may directly call the predict function in SentimentAnalysis class on any input sentence provided as a string as shown below. The function returns a value between 0 and 1. polarity = sentiment_cls.predict(\"The model is working well..\") Note: If the output value of the predict function is close to 0, it implies the statement has negative sentiment, whereas value close to 1 implies positive sentiment. If you run sentiment_analysis_example.py , you should get an output value of 0.9 which implies that the input statement The model is working well.. has strong positive sentiment.","title":"Getting Started"},{"location":"start/#getting-started","text":"","title":"Getting Started"},{"location":"start/#installation","text":"The installation of Auto-Keras is the same as other python packages. Note: currently, Auto-Keras is only compatible with: Python 3.6 .","title":"Installation"},{"location":"start/#latest-stable-version-pip-installation","text":"You can run the following pip installation command in your terminal to install the latest stable version. pip install autokeras","title":"Latest Stable Version (pip installation):"},{"location":"start/#bleeding-edge-version-manual-installation","text":"If you want to install the latest development version. You need to download the code from the GitHub repo and run the following commands in the project directory. pip install -r requirements.txt python setup.py install","title":"Bleeding Edge Version (manual installation):"},{"location":"start/#a-simple-example","text":"We show an example of image classification on the MNIST dataset, which is a famous benchmark image dataset for hand-written digits classification. Auto-Keras supports different types of data inputs.","title":"A Simple Example"},{"location":"start/#data-with-numpy-array-npy-format-source","text":"If the images and the labels are already formatted into numpy arrays, you can from keras.datasets import mnist from autokeras.image.image_supervised import ImageClassifier if __name__ == '__main__': (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(x_train.shape + (1,)) x_test = x_test.reshape(x_test.shape + (1,)) clf = ImageClassifier(verbose=True) clf.fit(x_train, y_train, time_limit=12 * 60 * 60) clf.final_fit(x_train, y_train, x_test, y_test, retrain=True) y = clf.evaluate(x_test, y_test) print(y) In the example above, the images and the labels are already formatted into numpy arrays.","title":"Data with numpy array (.npy) format. [source]"},{"location":"start/#what-if-your-data-are-raw-image-files-eg-jpg-png-bmp-source","text":"You can use our load_image_dataset function to load the images and their labels as follows. from autokeras.image.image_supervised import load_image_dataset x_train, y_train = load_image_dataset(csv_file_path=\"train/label.csv\", images_path=\"train\") print(x_train.shape) print(y_train.shape) x_test, y_test = load_image_dataset(csv_file_path=\"test/label.csv\", images_path=\"test\") print(x_test.shape) print(y_test.shape) The argument csv_file_path is the path to the CSV file containing the image file names and their corresponding labels. Both csv files and the raw image datasets could be downloaded from link . Here is an example of the csv file. File Name,Label 00000.jpg,5 00001.jpg,0 00002.jpg,4 00003.jpg,1 00004.jpg,9 00005.jpg,2 00006.jpg,1 ... The second argument images_path is the path to the directory containing all the images with those file names listed in the CSV file. The returned values x_train and y_train are the numpy arrays, which can be directly feed into the fit function of ImageClassifier . This CSV file for train or test can be created from folders containing images of a specific class (meaning label): train \u2514\u2500\u2500\u2500class_1 \u2502 \u2502 class_1_image_1.png \u2502 \u2502 class_1_image_2.png | | ... \u2514\u2500\u2500\u2500class_2 \u2502 class_2_image_1.png \u2502 class_2_image_2.png | ... The code below shows an example of how to create the CSV: train_dir = 'train' # Path to the train directory class_dirs = [i for i in os.listdir(path=train_dir) if os.path.isdir(os.path.join(train_dir, i))] with open('train/label.csv', 'w') as train_csv: fieldnames = ['File Name', 'Label'] writer = csv.DictWriter(train_csv, fieldnames=fieldnames) writer.writeheader() label = 0 for current_class in class_dirs: for image in os.listdir(os.path.join(train_dir, current_class)): writer.writerow({'File Name': str(image), 'Label':label}) label += 1 train_csv.close()","title":"What if your data are raw image files (e.g. .jpg, .png, .bmp)? [source]"},{"location":"start/#portable-models","text":"","title":"Portable Models"},{"location":"start/#how-to-export-keras-models","text":"clf.load_searcher().load_best_model().produce_keras_model().save('my_model.h5') This uses the keras function model.save() to export a single HDF5 file containing the architecture of the model, the weights of the model, the training configuration, and the state of the optimizer. See https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model Note: This is being built into AutoKeras as ImageClassifier().export_keras_model()","title":"How to export keras models?"},{"location":"start/#how-to-export-portable-model-source","text":"from autokeras import ImageClassifier clf = ImageClassifier(verbose=True, augment=False) clf.export_autokeras_model(model_file_name) The model will be stored into the path model_file_name .","title":"How to export Portable model? [source]"},{"location":"start/#how-to-load-exported-portable-model-source","text":"from autokeras.utils import pickle_from_file model = pickle_from_file(model_file_name) results = model.evaluate(x_test, y_test) print(results) The model will be loaded from the path model_file_name and then you can use the functions listed in PortableImageSupervised .","title":"How to load exported Portable model? [source]"},{"location":"start/#model-visualizations","text":"","title":"Model Visualizations"},{"location":"start/#how-to-visualize-keras-models","text":"This is not specific to AutoKeras, however, the following will generate a .PNG visualization of the best model found by AutoKeras: from keras.models import load_model model = load_model('my_model.h5') #See 'How to export keras models?' to generate this file before loading it. from keras.utils import plot_model plot_model(model, to_file='my_model.png')","title":"How to visualize keras models?"},{"location":"start/#how-to-visualize-the-best-selected-architecture-source","text":"While trying to create a model, let's say an Image classifier on MNIST, there is a facility for the user to visualize a .PDF depiction of the best architecture that was chosen by autokeras, after model training is complete. Prerequisites : 1) graphviz must be installed in your system. Refer Installation Guide 2) Additionally, also install \"graphviz\" python package using pip / conda pip: pip install graphviz conda : conda install -c conda-forge python-graphviz If the above installations are complete, proceed with the following steps : Step 1 : Specify a path before starting your model training clf = ImageClassifier(path=\"~/automodels/\",verbose=True, augment=False) # Give a custom path of your choice clf.fit(x_train, y_train, time_limit=30 * 60) clf.final_fit(x_train, y_train, x_test, y_test, retrain=True) Step 2 : After the model training is complete, run examples/visualize.py , whilst passing the same path as parameter if __name__ == '__main__': visualize('~/automodels/')","title":"How to visualize the best selected architecture? [source]"},{"location":"start/#net-modules","text":"","title":"Net Modules"},{"location":"start/#mlpmodule-tutorial-source","text":"MlpGenerator in net_module.py is a child class of Networkmodule . It can generates neural architecture with MLP modules Normally, there's two place to call the MlpGenerator, one is call MlpGenerator.fit while the other is MlpGenerator.final_fit . For example, in a image classification class ImageClassifier , one can initialize the cnn module as: mlpModule = MlpModule(loss, metric, searcher_args, path, verbose) Where: loss and metric determines by the type of training model(classification or regression or others) search_args can be referred in search.py path is the path to store the whole searching process and generated model. verbose is a boolean. Setting it to true prints to stdout. Then, for the searching part, one can call: mlpModule.fit(n_output_node, input_shape, train_data, test_data, time_limit=24 * 60 * 60) where: n_output_node: A integer value represent the number of output node in the final layer. input_shape: A tuple to express the shape of every train entry. For example, MNIST dataset would be (28,28,1). train_data: A PyTorch DataLoader instance representing the training data. test_data: A PyTorch DataLoader instance representing the testing data. * time_limit: A integer value represents the time limit on searching for models. And for final testing(testing the best searched model), one can call: mlpModule.final_fit(train_data, test_data, trainer_args=None, retrain=False) where: train_data: A DataLoader instance representing the training data. test_data: A DataLoader instance representing the testing data. trainer_args: A dictionary containing the parameters of the ModelTrainer constructor. retrain: A boolean of whether reinitialize the weights of the model.","title":"MlpModule tutorial. [source]"},{"location":"start/#cnnmodule-tutorial-source","text":"CnnGenerator in net_module.py is a child class of Networkmodule . It can generates neural architecture with basic cnn modules and the ResNet module. Normally, there's two place to call the CnnGenerator, one is call CnnGenerator.fit while the other is CnnGenerator.final_fit . For example, in a image classification class ImageClassifier , one can initialize the cnn module as: from autokeras import CnnModule from autokeras.nn.loss_function import classification_loss from autokeras.nn.metric import Accuracy TEST_FOLDER = \"test\" cnnModule = CnnModule(loss=classification_loss, metric=Accuracy, searcher_args={}, path=TEST_FOLDER, verbose=False) Where: loss and metric determines by the type of training model(classification or regression or others) search_args can be referred in search.py path is the path to store the whole searching process and generated model. verbose is a boolean. Setting it to true prints to stdout. Then, for the searching part, one can call: cnnModule.fit(n_output_node, input_shape, train_data, test_data, time_limit=24 * 60 * 60) where: n_output_node: A integer value represent the number of output node in the final layer. input_shape: A tuple to express the shape of every train entry. For example, MNIST dataset would be (28,28,1). train_data: A PyTorch DataLoader instance representing the training data. test_data: A PyTorch DataLoader instance representing the testing data. * time_limit: A integer value represents the time limit on searching for models. And for final testing(testing the best searched model), one can call: cnnModule.final_fit(train_data, test_data, trainer_args=None, retrain=False) where: train_data: A DataLoader instance representing the training data. test_data: A DataLoader instance representing the testing data. trainer_args: A dictionary containing the parameters of the ModelTrainer constructor. retrain: A boolean of whether reinitialize the weights of the model.","title":"CnnModule tutorial. [source]"},{"location":"start/#task-modules","text":"","title":"Task Modules"},{"location":"start/#automated-text-classifier-tutorial-source","text":"Class TextClassifier and TextRegressor are designed for automated generate best performance cnn neural architecture for a given text dataset. clf = TextClassifier(verbose=True) clf.fit(x=x_train, y=y_train, batch_size=10, time_limit=12 * 60 * 60) x_train: string format text data y_train: int format text label After searching the best model, one can call clf.final_fit to test the best model found in searching. Notes: Preprocessing of the text data: Class TextClassifier and TextRegressor contains a pre-process of the text data. Which means the input data should be in string format. The default pre-process model uses the glove6B model from Stanford NLP. * To change the default setting of the pre-process model, one need to change the corresponding variable: EMBEDDING_DIM , PRE_TRAIN_FILE_LINK , PRE_TRAIN_FILE_LINK , PRE_TRAIN_FILE_NAME in constant.py .","title":"Automated text classifier tutorial. [source]"},{"location":"start/#automated-tabular-classifier-tutorial-source","text":"Class TabularClassifier and TabularRegressor are designed for automated generate best performance shallow/deep architecture for a given tabular dataset. (Currently, theis module only supports lightgbm classifier and regressor.) clf = TabularClassifier(verbose=True) clf.fit(x_train, y_train, time_limit=12 * 60 * 60, data_info=datainfo) x_train: string format text data y_train: int format text label data_info: a numpy.array describing the feature types (time, numerical or categorical) of each column in x_train. Notes: Preprocessing of the tabular data: Class [TabularPreprocessor] involves several automated feature preprocessing and engineering operation for tabular data . The input data should be in numpy array format for the class TabularClassifier and TabularRegressor .","title":"Automated tabular classifier tutorial. [source]"},{"location":"start/#pretrained-models","text":"","title":"Pretrained Models"},{"location":"start/#object-detection-tutorial-source","text":"","title":"Object detection tutorial. [source]"},{"location":"start/#by-wuyang-chen-from-dr-atlas-wangs-group-at-cse-department-texas-am","text":"class_id_mapping = {0 : \"Business\", 1 : \"Sci/Tech\", 2 : \"Sports\", 3 : \"World\"} ObjectDetector in object_detector.py is a child class of Pretrained . Currently it can load a pretrained SSD model ( Liu, Wei, et al. \"Ssd: Single shot multibox detector.\" European conference on computer vision. Springer, Cham, 2016. ) and find object(s) in a given image. Let's first import the ObjectDetector and create a detection model ( detector ) with from autokeras.pretrained.object_detector import ObjectDetector detector = ObjectDetector() Note: the ObjectDetector class can automatically detect the existance of available cuda device(s), and use the device if exists. Second, you will want to load the pretrained weights for your model: detector.load() This line will automatically download and load the weights into detector . Finally you can make predictions against an image: results = detector.predict(\"/path/to/images/000001.jpg\", output_file_path=\"/path/to/images/\") Function detector.predict() requires the path to the image. If the output_file_path is not given, the detector will just return the numerical results as a list of dictionaries. Each dictionary is like {\"left\": int, \"top\": int, \"width\": int, \"height\": int: \"category\": str, \"confidence\": float}, where left and top is the (left, top) coordinates of the bounding box of the object and width and height are width and height of the box. category is a string representing the class the object belongs to, and the confidence can be regarded as the probability that the model believes its prediction is correct. If the output_file_path is given, then the results mentioned above will be plotted and saved in a new image file with suffix \"_prediction\" into the given output_file_path . If you run the example/object_detection/object_detection_example.py, you will get result [{'category': 'person', 'width': 331, 'height': 500, 'left': 17, 'confidence': 0.9741123914718628, 'top': 0}]","title":"by Wuyang Chen from Dr. Atlas Wang's group at CSE Department, Texas A&amp;M."},{"location":"start/#sentiment-analysis-tutorial-source","text":"The sentiment analysis module provides an interface to find the sentiment of any text input. The pretrained model is obtained by training Google AI\u2019s BERT model on IMDb dataset . Let\u2019s import the SentimentAnalysis module from sentiment_analysis.py . It is derived from the super class Pretrained . from autokeras.pretrained.text_sentiment import SentimentAnalysis sentiment_cls = SentimentAnalysis() During initialization of SentimentAnalysis , the pretrained model is loaded into memory i.e. CPU\u2019s or GPU\u2019s, if available. Now, you may directly call the predict function in SentimentAnalysis class on any input sentence provided as a string as shown below. The function returns a value between 0 and 1. polarity = sentiment_cls.predict(\"The model is working well..\") Note: If the output value of the predict function is close to 0, it implies the statement has negative sentiment, whereas value close to 1 implies positive sentiment. If you run sentiment_analysis_example.py , you should get an output value of 0.9 which implies that the input statement The model is working well.. has strong positive sentiment.","title":"Sentiment Analysis tutorial. [source]"},{"location":"temp/base/","text":"Pretrained The base class for all pretrained task. Attributes verbose : A boolean value indicating the verbosity mode. init Initialize the instance. load load pretrained model into self.model predict Return predict results for the given image Args: x_predict: An instance of numpy.ndarray containing the testing data. Returns: A numpy.ndarray containing the results.","title":"Base"},{"location":"temp/base/#pretrained","text":"The base class for all pretrained task.","title":"Pretrained"},{"location":"temp/base/#attributes","text":"verbose : A boolean value indicating the verbosity mode.","title":"Attributes"},{"location":"temp/base/#init","text":"Initialize the instance.","title":"init"},{"location":"temp/base/#load","text":"load pretrained model into self.model","title":"load"},{"location":"temp/base/#predict","text":"Return predict results for the given image Args: x_predict: An instance of numpy.ndarray containing the testing data. Returns: A numpy.ndarray containing the results.","title":"predict"},{"location":"temp/bayesian/","text":"layer_distance The distance between two layers. layers_distance The distance between the layers of two neural networks. skip_connection_distance The distance between two skip-connections. skip_connections_distance The distance between the skip-connections of two neural networks. edit_distance The distance between two neural networks. Args: x: An instance of NetworkDescriptor. y: An instance of NetworkDescriptor Returns edit_distance_matrix Calculate the edit distance. Args train_x : A list of neural architectures. train_y : A list of neural architectures. Returns vector_distance The Euclidean distance between two vectors. bourgain_embedding_matrix Use Bourgain algorithm to embed the neural architectures based on their edit-distance. Args distance_matrix : A matrix of edit-distances. Returns contain Check if the target descriptor is in the descriptors. IncrementalGaussianProcess Gaussian process regressor. Attributes alpha : A hyperparameter. fit Fit the regressor with more data. Args train_x : A list of NetworkDescriptor. train_y : A list of metric values. incremental_fit Incrementally fit the regressor. first_fit Fit the regressor for the first time. predict Predict the result. Args train_x : A list of NetworkDescriptor. Returns y_mean : The predicted mean. y_std : The predicted standard deviation. BayesianOptimizer A Bayesian optimizer for neural architectures. Attributes searcher : The Searcher which is calling the Bayesian optimizer. t_min : The minimum temperature for simulated annealing. metric : An instance of the Metric subclasses. gpr : A GaussianProcessRegressor for bayesian optimization. beta : The beta in acquisition function. (refer to our paper) search_tree : The network morphism search tree. fit Fit the optimizer with new architectures and performances. Args x_queue : A list of NetworkDescriptor. y_queue : A list of metric values. generate Generate new architecture. Args descriptors : All the searched neural architectures. timeout : An integer. The time limit in seconds. sync_message : the Queue for multiprocessing return value. Returns graph : An instance of Graph. A morphed neural network with weights. father_id : The father node ID in the search tree. Elem Elements to be sorted according to metric value. ReverseElem Elements to be reversely sorted according to metric value. SearchTree The network morphism search tree. get_dict A recursive function to return the content of the tree in a dict.","title":"bayesian"},{"location":"temp/bayesian/#layer_distance","text":"The distance between two layers.","title":"layer_distance"},{"location":"temp/bayesian/#layers_distance","text":"The distance between the layers of two neural networks.","title":"layers_distance"},{"location":"temp/bayesian/#skip_connection_distance","text":"The distance between two skip-connections.","title":"skip_connection_distance"},{"location":"temp/bayesian/#skip_connections_distance","text":"The distance between the skip-connections of two neural networks.","title":"skip_connections_distance"},{"location":"temp/bayesian/#edit_distance","text":"The distance between two neural networks. Args: x: An instance of NetworkDescriptor. y: An instance of NetworkDescriptor","title":"edit_distance"},{"location":"temp/bayesian/#returns","text":"","title":"Returns"},{"location":"temp/bayesian/#edit_distance_matrix","text":"Calculate the edit distance.","title":"edit_distance_matrix"},{"location":"temp/bayesian/#args","text":"train_x : A list of neural architectures. train_y : A list of neural architectures.","title":"Args"},{"location":"temp/bayesian/#returns_1","text":"","title":"Returns"},{"location":"temp/bayesian/#vector_distance","text":"The Euclidean distance between two vectors.","title":"vector_distance"},{"location":"temp/bayesian/#bourgain_embedding_matrix","text":"Use Bourgain algorithm to embed the neural architectures based on their edit-distance.","title":"bourgain_embedding_matrix"},{"location":"temp/bayesian/#args_1","text":"distance_matrix : A matrix of edit-distances.","title":"Args"},{"location":"temp/bayesian/#returns_2","text":"","title":"Returns"},{"location":"temp/bayesian/#contain","text":"Check if the target descriptor is in the descriptors.","title":"contain"},{"location":"temp/bayesian/#incrementalgaussianprocess","text":"Gaussian process regressor.","title":"IncrementalGaussianProcess"},{"location":"temp/bayesian/#attributes","text":"alpha : A hyperparameter.","title":"Attributes"},{"location":"temp/bayesian/#fit","text":"Fit the regressor with more data.","title":"fit"},{"location":"temp/bayesian/#args_2","text":"train_x : A list of NetworkDescriptor. train_y : A list of metric values.","title":"Args"},{"location":"temp/bayesian/#incremental_fit","text":"Incrementally fit the regressor.","title":"incremental_fit"},{"location":"temp/bayesian/#first_fit","text":"Fit the regressor for the first time.","title":"first_fit"},{"location":"temp/bayesian/#predict","text":"Predict the result.","title":"predict"},{"location":"temp/bayesian/#args_3","text":"train_x : A list of NetworkDescriptor.","title":"Args"},{"location":"temp/bayesian/#returns_3","text":"y_mean : The predicted mean. y_std : The predicted standard deviation.","title":"Returns"},{"location":"temp/bayesian/#bayesianoptimizer","text":"A Bayesian optimizer for neural architectures.","title":"BayesianOptimizer"},{"location":"temp/bayesian/#attributes_1","text":"searcher : The Searcher which is calling the Bayesian optimizer. t_min : The minimum temperature for simulated annealing. metric : An instance of the Metric subclasses. gpr : A GaussianProcessRegressor for bayesian optimization. beta : The beta in acquisition function. (refer to our paper) search_tree : The network morphism search tree.","title":"Attributes"},{"location":"temp/bayesian/#fit_1","text":"Fit the optimizer with new architectures and performances.","title":"fit"},{"location":"temp/bayesian/#args_4","text":"x_queue : A list of NetworkDescriptor. y_queue : A list of metric values.","title":"Args"},{"location":"temp/bayesian/#generate","text":"Generate new architecture.","title":"generate"},{"location":"temp/bayesian/#args_5","text":"descriptors : All the searched neural architectures. timeout : An integer. The time limit in seconds. sync_message : the Queue for multiprocessing return value.","title":"Args"},{"location":"temp/bayesian/#returns_4","text":"graph : An instance of Graph. A morphed neural network with weights. father_id : The father node ID in the search tree.","title":"Returns"},{"location":"temp/bayesian/#elem","text":"Elements to be sorted according to metric value.","title":"Elem"},{"location":"temp/bayesian/#reverseelem","text":"Elements to be reversely sorted according to metric value.","title":"ReverseElem"},{"location":"temp/bayesian/#searchtree","text":"The network morphism search tree.","title":"SearchTree"},{"location":"temp/bayesian/#get_dict","text":"A recursive function to return the content of the tree in a dict.","title":"get_dict"},{"location":"temp/builder/","text":"deepvoice3 Build deepvoice3","title":"Builder"},{"location":"temp/builder/#deepvoice3","text":"Build deepvoice3","title":"deepvoice3"},{"location":"temp/cleaners/","text":"english_cleaners Pipeline for English text, including number and abbreviation expansion.","title":"Cleaners"},{"location":"temp/cleaners/#english_cleaners","text":"Pipeline for English text, including number and abbreviation expansion.","title":"english_cleaners"},{"location":"temp/cmudict/","text":"","title":"Cmudict"},{"location":"temp/constant/","text":"","title":"Constant"},{"location":"temp/contribute/","text":"Contributing Guide Contributions are welcome, and greatly appreciated! Every little bit helps, and credit will always be given. We recommend you to check our Developer Tools Guide to make the development process easier and standard. Notably, you can follow the tag of call for contributors in the issues. Those issues are designed for the external contributors to solve. The pull requests solving these issues are most likely to be merged. There are many ways to contribute to Auto-Keras, including submit feedback, fix bugs, implement features, and write documentation. The guide for each type of contribution is as follows. Submit Feedback The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels. Fix Bugs: You may look through the GitHub issues for bugs. Anything tagged with \"bug report\" is open to whoever wants to implement it. Please follow the Pull Request Guide to submit your pull request. Please also read Code Style Guide , and Documentation Guide to ensure your merge request meet our requirements. Implement Features You may look through the GitHub issues for feature requests. Anything tagged with \"feature request\" is open to whoever wants to implement it. Please follow the Pull Request Guide to submit your pull request. Please also read Code Style Guide , Documentation Guide , and Testing Guide to ensure your merge request meet our requirements. Write Documentation The documentation of Auto-Keras is either directly written into the Markdown files in mkdocs directory , or automatically extracted from the docstrings by executing the autogen.py . In the first situation, you only need to change the markdown file. In the second situation, you need to change the docstrings and execute autogen.py to update the Markdown files. Please follow the Pull Request Guide to submit your pull request. Please also read Documentation Guide to ensure your merge request meet our requirements. Pull Request Guide Before you submit a pull request, check that it meets these guidelines: Fork the repository. Create a new branch from the master branch. Give your new branch a meaningful name. Pull request from your new branch to the master branch of the original autokeras repo. Give your pull request a meaningful name. Include \"resolves #issue_number\" in the description of the pull request and briefly describe your contribution. Submit the pull request from the first day of your development (after your first commit) and prefix the title of the pull request with [WIP] . When the contribution is complete, make sure the pull request passed the CI tests. Change the [WIP] to [MRG] . Set the reviewer to @jhfjhfj1 . For the case of bug fixes, add new test cases which would fail before your bug fix. If you are a collaborator of the autokeras repo, you don't need to fork the repository. Just create a new branch directly. You also need to change the assignee to the reviewer when request for code review. The reviewer will change the assignee back to you when finished the review. The assignee always means who should push the progress of the pull request now. Code Style Guide This project tries to closely follow the official Python Style Guide detailed in PEP8 . The docstrings follow the Google Python Style Guide . Please follow these style guide closely, especially for the docstrings, which would be extracted automatically to generate the documentation. Documentation Guide: The documentation should be provided in two ways, docstring, tutorial, and readme file. We prefer the documentation to be as complete as possible. Docstring All the methods and classes may directly be called by the user need to be documented with docstrings. The docstrings should contain all the fields required by the Google Python Style Guide . Tutorial You only need to add tutorials to your code if you are contributing or updating a new task module, e.g. TextClassifier, VideoClassifier, or a new function could be directly called by the user. You can modify mkdocs/docs/start.md to add your tutorial. The code example of your new task module should be added to the examples directory. Readme File You only need to add tutorials to your code if you are contributing or updating a new task module, e.g. TextClassifier, VideoClassifier. The readme file should be named as README.md . It should be written in Markdown. The content should contain your name, affiliation, and any reference to the method you use. Testing Guide Pytest is used to write the unit tests of Auto-Keras. You should test your code by writing unit testing code in tests directory. The testing file name should be the .py file with a prefix of test_ in the corresponding directory, e.g., the name should be test_layers.py if the code of which is to test layer.py . The tests should be run in the root directory of the project by executing the cov.sh file. It would output the coverage information into a directory named htmlcov . Please make sure the code coverage percentage does not decrease after your contribution, otherwise, the code will not be merged. Developer Tools Guide We highly recommend you to use Pycharm and virtualenvwrapper . Pycharm Pycharm is the best IDE for large project development in Python. We recommend you inspect the code before you pull request to fix any error and warning suggested by the inspection. Virtualenvwrapper Virtualenvwrapper is a tool to build separated Python environment for each project. In this way, you can install a different version of Tensorflow, Pytorch, or any other package for each project. We recommend you to create a virtualenv for autokeras development with virtualenvwrapper, and only install the packages required by autokeras with the corresponding version. The virtualenv should be created based on Python 3.6 interpreter. Use pycharm to select the virtualenv as interpreter . Reusable Code Guide You may checkout this code review video to get familiar with the code structure. Other than the base classes you have to extend, there are some other classes you can extend. ModelTrainer autokeras.model_trainer.ModelTrainer is a class for training Pytorch models. If needed a new metric or loss function other than the ones we have, you can add your own to loss_function.py and metric.py . You can follow its documentation and this example to use it. Make sure your loss function, metric, Pytorch model, and Dataloader are compatible with each other. Main Contributor List We really appreciate all the contributions. To show our appreciation to those who contributed most, we would like to maintain a list of main contributors. To be in the list, you need to meet the following requirments. 1. Be on campus of Texas A&M University. 2. Constantly present in our meetings. 3. Constantly contribute code to our repository. 4. Keep the above for over 6 months.","title":"Contributing Guide"},{"location":"temp/contribute/#contributing-guide","text":"Contributions are welcome, and greatly appreciated! Every little bit helps, and credit will always be given. We recommend you to check our Developer Tools Guide to make the development process easier and standard. Notably, you can follow the tag of call for contributors in the issues. Those issues are designed for the external contributors to solve. The pull requests solving these issues are most likely to be merged. There are many ways to contribute to Auto-Keras, including submit feedback, fix bugs, implement features, and write documentation. The guide for each type of contribution is as follows.","title":"Contributing Guide"},{"location":"temp/contribute/#submit-feedback","text":"The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels.","title":"Submit Feedback"},{"location":"temp/contribute/#fix-bugs","text":"You may look through the GitHub issues for bugs. Anything tagged with \"bug report\" is open to whoever wants to implement it. Please follow the Pull Request Guide to submit your pull request. Please also read Code Style Guide , and Documentation Guide to ensure your merge request meet our requirements.","title":"Fix Bugs:"},{"location":"temp/contribute/#implement-features","text":"You may look through the GitHub issues for feature requests. Anything tagged with \"feature request\" is open to whoever wants to implement it. Please follow the Pull Request Guide to submit your pull request. Please also read Code Style Guide , Documentation Guide , and Testing Guide to ensure your merge request meet our requirements.","title":"Implement Features"},{"location":"temp/contribute/#write-documentation","text":"The documentation of Auto-Keras is either directly written into the Markdown files in mkdocs directory , or automatically extracted from the docstrings by executing the autogen.py . In the first situation, you only need to change the markdown file. In the second situation, you need to change the docstrings and execute autogen.py to update the Markdown files. Please follow the Pull Request Guide to submit your pull request. Please also read Documentation Guide to ensure your merge request meet our requirements.","title":"Write Documentation"},{"location":"temp/contribute/#pull-request-guide","text":"Before you submit a pull request, check that it meets these guidelines: Fork the repository. Create a new branch from the master branch. Give your new branch a meaningful name. Pull request from your new branch to the master branch of the original autokeras repo. Give your pull request a meaningful name. Include \"resolves #issue_number\" in the description of the pull request and briefly describe your contribution. Submit the pull request from the first day of your development (after your first commit) and prefix the title of the pull request with [WIP] . When the contribution is complete, make sure the pull request passed the CI tests. Change the [WIP] to [MRG] . Set the reviewer to @jhfjhfj1 . For the case of bug fixes, add new test cases which would fail before your bug fix. If you are a collaborator of the autokeras repo, you don't need to fork the repository. Just create a new branch directly. You also need to change the assignee to the reviewer when request for code review. The reviewer will change the assignee back to you when finished the review. The assignee always means who should push the progress of the pull request now.","title":"Pull Request Guide"},{"location":"temp/contribute/#code-style-guide","text":"This project tries to closely follow the official Python Style Guide detailed in PEP8 . The docstrings follow the Google Python Style Guide . Please follow these style guide closely, especially for the docstrings, which would be extracted automatically to generate the documentation.","title":"Code Style Guide"},{"location":"temp/contribute/#documentation-guide","text":"The documentation should be provided in two ways, docstring, tutorial, and readme file. We prefer the documentation to be as complete as possible.","title":"Documentation Guide:"},{"location":"temp/contribute/#docstring","text":"All the methods and classes may directly be called by the user need to be documented with docstrings. The docstrings should contain all the fields required by the Google Python Style Guide .","title":"Docstring"},{"location":"temp/contribute/#tutorial","text":"You only need to add tutorials to your code if you are contributing or updating a new task module, e.g. TextClassifier, VideoClassifier, or a new function could be directly called by the user. You can modify mkdocs/docs/start.md to add your tutorial. The code example of your new task module should be added to the examples directory.","title":"Tutorial"},{"location":"temp/contribute/#readme-file","text":"You only need to add tutorials to your code if you are contributing or updating a new task module, e.g. TextClassifier, VideoClassifier. The readme file should be named as README.md . It should be written in Markdown. The content should contain your name, affiliation, and any reference to the method you use.","title":"Readme File"},{"location":"temp/contribute/#testing-guide","text":"Pytest is used to write the unit tests of Auto-Keras. You should test your code by writing unit testing code in tests directory. The testing file name should be the .py file with a prefix of test_ in the corresponding directory, e.g., the name should be test_layers.py if the code of which is to test layer.py . The tests should be run in the root directory of the project by executing the cov.sh file. It would output the coverage information into a directory named htmlcov . Please make sure the code coverage percentage does not decrease after your contribution, otherwise, the code will not be merged.","title":"Testing Guide"},{"location":"temp/contribute/#developer-tools-guide","text":"We highly recommend you to use Pycharm and virtualenvwrapper .","title":"Developer Tools Guide"},{"location":"temp/contribute/#pycharm","text":"Pycharm is the best IDE for large project development in Python. We recommend you inspect the code before you pull request to fix any error and warning suggested by the inspection.","title":"Pycharm"},{"location":"temp/contribute/#virtualenvwrapper","text":"Virtualenvwrapper is a tool to build separated Python environment for each project. In this way, you can install a different version of Tensorflow, Pytorch, or any other package for each project. We recommend you to create a virtualenv for autokeras development with virtualenvwrapper, and only install the packages required by autokeras with the corresponding version. The virtualenv should be created based on Python 3.6 interpreter. Use pycharm to select the virtualenv as interpreter .","title":"Virtualenvwrapper"},{"location":"temp/contribute/#reusable-code-guide","text":"You may checkout this code review video to get familiar with the code structure. Other than the base classes you have to extend, there are some other classes you can extend.","title":"Reusable Code Guide"},{"location":"temp/contribute/#modeltrainer","text":"autokeras.model_trainer.ModelTrainer is a class for training Pytorch models. If needed a new metric or loss function other than the ones we have, you can add your own to loss_function.py and metric.py . You can follow its documentation and this example to use it. Make sure your loss function, metric, Pytorch model, and Dataloader are compatible with each other.","title":"ModelTrainer"},{"location":"temp/contribute/#main-contributor-list","text":"We really appreciate all the contributions. To show our appreciation to those who contributed most, we would like to maintain a list of main contributors. To be in the list, you need to meet the following requirments. 1. Be on campus of Texas A&M University. 2. Constantly present in our meetings. 3. Constantly contribute code to our repository. 4. Keep the above for over 6 months.","title":"Main Contributor List"},{"location":"temp/conv/","text":"Conv1d Extended nn.Conv1d for incremental dilated convolutions","title":"Conv"},{"location":"temp/conv/#conv1d","text":"Extended nn.Conv1d for incremental dilated convolutions","title":"Conv1d"},{"location":"temp/deepvoice3/","text":"","title":"Deepvoice3"},{"location":"temp/face_detector/","text":"FaceDetector A class to predict faces using the MTCNN pre-trained model. predict Predicts faces in an image. Args img_path : A string. The path to the image on which the prediction is to be done. output_file_path : A string. The path where the output image is to be saved after the prediction. None by default. Returns","title":"Face detector"},{"location":"temp/face_detector/#facedetector","text":"A class to predict faces using the MTCNN pre-trained model.","title":"FaceDetector"},{"location":"temp/face_detector/#predict","text":"Predicts faces in an image.","title":"predict"},{"location":"temp/face_detector/#args","text":"img_path : A string. The path to the image on which the prediction is to be done. output_file_path : A string. The path where the output image is to be saved after the prediction. None by default.","title":"Args"},{"location":"temp/face_detector/#returns","text":"","title":"Returns"},{"location":"temp/file_utils/","text":"url_to_filename Convert url into a hashed filename in a repeatable way. If etag is specified, append its hash to the url's, delimited by a period. cached_path Given something that might be a URL (or might be a local path), determine which. If it's a URL, download the file and cache it, and return the path to the cached file. If it's already a local path, make sure the file exists and then return the path. split_s3_path Split a full s3 path into the bucket name and path. s3_request Wrapper function for s3 requests in order to create more helpful error messages. s3_etag Check ETag on S3 object. s3_get Pull a file directly from S3. get_from_cache Given a URL, look for the corresponding dataset in the local cache. If it's not there, download it. Then return the path to the cached file.","title":"File utils"},{"location":"temp/file_utils/#url_to_filename","text":"Convert url into a hashed filename in a repeatable way. If etag is specified, append its hash to the url's, delimited by a period.","title":"url_to_filename"},{"location":"temp/file_utils/#cached_path","text":"Given something that might be a URL (or might be a local path), determine which. If it's a URL, download the file and cache it, and return the path to the cached file. If it's already a local path, make sure the file exists and then return the path.","title":"cached_path"},{"location":"temp/file_utils/#split_s3_path","text":"Split a full s3 path into the bucket name and path.","title":"split_s3_path"},{"location":"temp/file_utils/#s3_request","text":"Wrapper function for s3 requests in order to create more helpful error messages.","title":"s3_request"},{"location":"temp/file_utils/#s3_etag","text":"Check ETag on S3 object.","title":"s3_etag"},{"location":"temp/file_utils/#s3_get","text":"Pull a file directly from S3.","title":"s3_get"},{"location":"temp/file_utils/#get_from_cache","text":"Given a URL, look for the corresponding dataset in the local cache. If it's not there, download it. Then return the path to the cached file.","title":"get_from_cache"},{"location":"temp/frontend/","text":"","title":"Frontend"},{"location":"temp/gan/","text":"DCGAN Deep Convolution Generative Adversary Network init Args: nz: size of the latent z vector ngf: of gen filters in first conv layer ndf: of discrim filters in first conv layer nc: number of input chanel verbose: A boolean of whether the search process will be printed to stdout. gen_training_result: A tuple of (path, size) to denote where to output the intermediate result with size augment: A boolean value indicating whether the data needs augmentation. fit Train only Args x_train : ndarray contained the training data","title":"Gan"},{"location":"temp/gan/#dcgan","text":"Deep Convolution Generative Adversary Network","title":"DCGAN"},{"location":"temp/gan/#init","text":"Args: nz: size of the latent z vector ngf: of gen filters in first conv layer ndf: of discrim filters in first conv layer nc: number of input chanel verbose: A boolean of whether the search process will be printed to stdout. gen_training_result: A tuple of (path, size) to denote where to output the intermediate result with size augment: A boolean value indicating whether the data needs augmentation.","title":"init"},{"location":"temp/gan/#fit","text":"Train only","title":"fit"},{"location":"temp/gan/#args","text":"x_train : ndarray contained the training data","title":"Args"},{"location":"temp/generator/","text":"NetworkGenerator The base class for generating a network. It can be used to generate a CNN or Multi-Layer Perceptron. Attributes n_output_node : Number of output nodes in the network. input_shape : A tuple to represent the input shape. init Initialize the instance. Sets the parameters n_output_node and input_shape for the instance. Args n_output_node : An integer. Number of output nodes in the network. input_shape : A tuple. Input shape of the network. CnnGenerator A class to generate CNN. Attributes n_dim : len(self.input_shape) - 1 conv : A class that represents (n_dim-1) dimensional convolution. dropout : A class that represents (n_dim-1) dimensional dropout. global_avg_pooling : A class that represents (n_dim-1) dimensional Global Average Pooling. pooling : A class that represents (n_dim-1) dimensional pooling. batch_norm : A class that represents (n_dim-1) dimensional batch normalization. init Initialize the instance. Args n_output_node : An integer. Number of output nodes in the network. input_shape : A tuple. Input shape of the network. generate Generates a CNN. Args model_len : An integer. Number of convolutional layers. model_width : An integer. Number of filters for the convolutional layers. Returns MlpGenerator A class to generate Multi-Layer Perceptron. init Initialize the instance. Args n_output_node : An integer. Number of output nodes in the network. input_shape : A tuple. Input shape of the network. If it is 1D, ensure the value is appended by a comma in the tuple. generate Generates a Multi-Layer Perceptron. Args model_len : An integer. Number of hidden layers. model_width : An integer or a list of integers of length model_len . If it is a list, it represents the number of nodes in each hidden layer. If it is an integer, all hidden layers have nodes equal to this value. Returns","title":"generator"},{"location":"temp/generator/#networkgenerator","text":"The base class for generating a network. It can be used to generate a CNN or Multi-Layer Perceptron.","title":"NetworkGenerator"},{"location":"temp/generator/#attributes","text":"n_output_node : Number of output nodes in the network. input_shape : A tuple to represent the input shape.","title":"Attributes"},{"location":"temp/generator/#init","text":"Initialize the instance. Sets the parameters n_output_node and input_shape for the instance.","title":"init"},{"location":"temp/generator/#args","text":"n_output_node : An integer. Number of output nodes in the network. input_shape : A tuple. Input shape of the network.","title":"Args"},{"location":"temp/generator/#cnngenerator","text":"A class to generate CNN.","title":"CnnGenerator"},{"location":"temp/generator/#attributes_1","text":"n_dim : len(self.input_shape) - 1 conv : A class that represents (n_dim-1) dimensional convolution. dropout : A class that represents (n_dim-1) dimensional dropout. global_avg_pooling : A class that represents (n_dim-1) dimensional Global Average Pooling. pooling : A class that represents (n_dim-1) dimensional pooling. batch_norm : A class that represents (n_dim-1) dimensional batch normalization.","title":"Attributes"},{"location":"temp/generator/#init_1","text":"Initialize the instance.","title":"init"},{"location":"temp/generator/#args_1","text":"n_output_node : An integer. Number of output nodes in the network. input_shape : A tuple. Input shape of the network.","title":"Args"},{"location":"temp/generator/#generate","text":"Generates a CNN.","title":"generate"},{"location":"temp/generator/#args_2","text":"model_len : An integer. Number of convolutional layers. model_width : An integer. Number of filters for the convolutional layers.","title":"Args"},{"location":"temp/generator/#returns","text":"","title":"Returns"},{"location":"temp/generator/#mlpgenerator","text":"A class to generate Multi-Layer Perceptron.","title":"MlpGenerator"},{"location":"temp/generator/#init_2","text":"Initialize the instance.","title":"init"},{"location":"temp/generator/#args_3","text":"n_output_node : An integer. Number of output nodes in the network. input_shape : A tuple. Input shape of the network. If it is 1D, ensure the value is appended by a comma in the tuple.","title":"Args"},{"location":"temp/generator/#generate_1","text":"Generates a Multi-Layer Perceptron.","title":"generate"},{"location":"temp/generator/#args_4","text":"model_len : An integer. Number of hidden layers. model_width : An integer or a list of integers of length model_len . If it is a list, it represents the number of nodes in each hidden layer. If it is an integer, all hidden layers have nodes equal to this value.","title":"Args"},{"location":"temp/generator/#returns_1","text":"","title":"Returns"},{"location":"temp/graph/","text":"NetworkDescriptor A class describing the neural architecture for neural network kernel. It only record the width of convolutional and dense layers, and the skip-connection types and positions. add_skip_connection Add a skip-connection to the descriptor. Args u : Number of convolutional layers before the starting point. v : Number of convolutional layers before the ending point. connection_type : Must be either CONCAT_CONNECT or ADD_CONNECT. Node A class for intermediate output tensor (node) in the Graph. Attributes shape : A tuple describing the shape of the tensor. Graph A class representing the neural architecture graph of a Keras model. Graph extracts the neural architecture graph from a Keras model. Each node in the graph is a intermediate tensor between layers. Each layer is an edge in the graph. Notably, multiple edges may refer to the same layer. (e.g. Add layer is adding two tensor into one tensor. So it is related to two edges.) Attributes input_shape : A tuple describing the input tensor shape, not including the number of instances. weighted : A boolean marking if there are actual values in the weights of the layers. Sometime we only need the neural architecture information with a graph. In that case, we do not save the weights to save memory and time. node_list : A list of integers. The indices of the list are the identifiers. layer_list : A list of stub layers. The indices of the list are the identifiers. node_to_id : A dict instance mapping from node integers to their identifiers. layer_to_id : A dict instance mapping from stub layers to their identifiers. layer_id_to_input_node_ids : A dict instance mapping from layer identifiers to their input nodes identifiers. layer_id_to_output_node_ids : A dict instance mapping from layer identifiers to their output nodes identifiers. adj_list : A two dimensional list. The adjacency list of the graph. The first dimension is identified by tensor identifiers. In each edge list, the elements are two-element tuples of (tensor identifier, layer identifier). reverse_adj_list : A reverse adjacent list in the same format as adj_list. operation_history : A list saving all the network morphism operations. n_dim : An integer. If it uses Conv1d, n_dim should be 1. vis : A dictionary of temporary storage for whether an local operation has been done during the network morphism. init Initializer for Graph. Args input_shape : A tuple describing the input tensor shape, not including the number of instances. weighted : A boolean marking if there are actual values in the weights of the layers. Sometime we only need the neural architecture information with a graph. In that case, we do not save the weights to save memory and time. add_layer Add a layer to the Graph. Args layer : An instance of the subclasses of StubLayer in layers.py. input_node_id : An integer. The ID of the input node of the layer. Returns output_node_id : An integer. The ID of the output node of the layer. n_nodes Return the number of nodes in the model. n_layers Return the number of layers in the model. _add_node Add a new node to node_list and give the node an ID. Args node : An instance of Node. Returns node_id : An integer. _add_edge Add a new layer to the graph. The nodes should be created in advance. _redirect_edge Redirect the layer to a new node. Change the edge originally from u_id to v_id into an edge from u_id to new_v_id while keeping all other property of the edge the same. _replace_layer Replace the layer with a new layer. topological_order Return the topological order of the node IDs from the input node to the output node. _get_pooling_layers Given two node IDs, return all the pooling layers between them. _depth_first_search Search for all the layers and nodes down the path. A recursive function to search all the layers and nodes between the node in the node_list and the node with target_id. _search Search the graph for all the layers to be widened caused by an operation. It is an recursive function with duplication check to avoid deadlock. It searches from a starting node u until the corresponding layers has been widened. Args u : The starting node ID. start_dim : The position to insert the additional dimensions. total_dim : The total number of dimensions the layer has before widening. n_add : The number of dimensions to add. to_deeper_model Insert a relu-conv-bn block after the target block. Args target_id : A convolutional layer ID. The new block should be inserted after the block. new_layer : An instance of StubLayer subclasses. to_wider_model Widen the last dimension of the output of the pre_layer. Args pre_layer_id : The ID of a convolutional layer or dense layer. n_add : The number of dimensions to add. _insert_new_layers Insert the new_layers after the node with start_node_id. to_add_skip_model Add a weighted add skip-connection from after start node to end node. Args start_id : The convolutional layer ID, after which to start the skip-connection. end_id : The convolutional layer ID, after which to end the skip-connection. to_concat_skip_model Add a weighted add concatenate connection from after start node to end node. Args start_id : The convolutional layer ID, after which to start the skip-connection. end_id : The convolutional layer ID, after which to end the skip-connection. extract_descriptor Extract the the description of the Graph as an instance of NetworkDescriptor. produce_model Build a new torch model based on the current graph. produce_keras_model Build a new keras model based on the current graph. get_main_chain_layers Return a list of layer IDs in the main chain. get_main_chain Returns the main chain node ID list. TorchModel A neural network class using pytorch constructed from an instance of Graph.","title":"graph"},{"location":"temp/graph/#networkdescriptor","text":"A class describing the neural architecture for neural network kernel. It only record the width of convolutional and dense layers, and the skip-connection types and positions.","title":"NetworkDescriptor"},{"location":"temp/graph/#add_skip_connection","text":"Add a skip-connection to the descriptor.","title":"add_skip_connection"},{"location":"temp/graph/#args","text":"u : Number of convolutional layers before the starting point. v : Number of convolutional layers before the ending point. connection_type : Must be either CONCAT_CONNECT or ADD_CONNECT.","title":"Args"},{"location":"temp/graph/#node","text":"A class for intermediate output tensor (node) in the Graph.","title":"Node"},{"location":"temp/graph/#attributes","text":"shape : A tuple describing the shape of the tensor.","title":"Attributes"},{"location":"temp/graph/#graph","text":"A class representing the neural architecture graph of a Keras model. Graph extracts the neural architecture graph from a Keras model. Each node in the graph is a intermediate tensor between layers. Each layer is an edge in the graph. Notably, multiple edges may refer to the same layer. (e.g. Add layer is adding two tensor into one tensor. So it is related to two edges.)","title":"Graph"},{"location":"temp/graph/#attributes_1","text":"input_shape : A tuple describing the input tensor shape, not including the number of instances. weighted : A boolean marking if there are actual values in the weights of the layers. Sometime we only need the neural architecture information with a graph. In that case, we do not save the weights to save memory and time. node_list : A list of integers. The indices of the list are the identifiers. layer_list : A list of stub layers. The indices of the list are the identifiers. node_to_id : A dict instance mapping from node integers to their identifiers. layer_to_id : A dict instance mapping from stub layers to their identifiers. layer_id_to_input_node_ids : A dict instance mapping from layer identifiers to their input nodes identifiers. layer_id_to_output_node_ids : A dict instance mapping from layer identifiers to their output nodes identifiers. adj_list : A two dimensional list. The adjacency list of the graph. The first dimension is identified by tensor identifiers. In each edge list, the elements are two-element tuples of (tensor identifier, layer identifier). reverse_adj_list : A reverse adjacent list in the same format as adj_list. operation_history : A list saving all the network morphism operations. n_dim : An integer. If it uses Conv1d, n_dim should be 1. vis : A dictionary of temporary storage for whether an local operation has been done during the network morphism.","title":"Attributes"},{"location":"temp/graph/#init","text":"Initializer for Graph.","title":"init"},{"location":"temp/graph/#args_1","text":"input_shape : A tuple describing the input tensor shape, not including the number of instances. weighted : A boolean marking if there are actual values in the weights of the layers. Sometime we only need the neural architecture information with a graph. In that case, we do not save the weights to save memory and time.","title":"Args"},{"location":"temp/graph/#add_layer","text":"Add a layer to the Graph.","title":"add_layer"},{"location":"temp/graph/#args_2","text":"layer : An instance of the subclasses of StubLayer in layers.py. input_node_id : An integer. The ID of the input node of the layer.","title":"Args"},{"location":"temp/graph/#returns","text":"output_node_id : An integer. The ID of the output node of the layer.","title":"Returns"},{"location":"temp/graph/#n_nodes","text":"Return the number of nodes in the model.","title":"n_nodes"},{"location":"temp/graph/#n_layers","text":"Return the number of layers in the model.","title":"n_layers"},{"location":"temp/graph/#_add_node","text":"Add a new node to node_list and give the node an ID.","title":"_add_node"},{"location":"temp/graph/#args_3","text":"node : An instance of Node.","title":"Args"},{"location":"temp/graph/#returns_1","text":"node_id : An integer.","title":"Returns"},{"location":"temp/graph/#_add_edge","text":"Add a new layer to the graph. The nodes should be created in advance.","title":"_add_edge"},{"location":"temp/graph/#_redirect_edge","text":"Redirect the layer to a new node. Change the edge originally from u_id to v_id into an edge from u_id to new_v_id while keeping all other property of the edge the same.","title":"_redirect_edge"},{"location":"temp/graph/#_replace_layer","text":"Replace the layer with a new layer.","title":"_replace_layer"},{"location":"temp/graph/#topological_order","text":"Return the topological order of the node IDs from the input node to the output node.","title":"topological_order"},{"location":"temp/graph/#_get_pooling_layers","text":"Given two node IDs, return all the pooling layers between them.","title":"_get_pooling_layers"},{"location":"temp/graph/#_depth_first_search","text":"Search for all the layers and nodes down the path. A recursive function to search all the layers and nodes between the node in the node_list and the node with target_id.","title":"_depth_first_search"},{"location":"temp/graph/#_search","text":"Search the graph for all the layers to be widened caused by an operation. It is an recursive function with duplication check to avoid deadlock. It searches from a starting node u until the corresponding layers has been widened.","title":"_search"},{"location":"temp/graph/#args_4","text":"u : The starting node ID. start_dim : The position to insert the additional dimensions. total_dim : The total number of dimensions the layer has before widening. n_add : The number of dimensions to add.","title":"Args"},{"location":"temp/graph/#to_deeper_model","text":"Insert a relu-conv-bn block after the target block.","title":"to_deeper_model"},{"location":"temp/graph/#args_5","text":"target_id : A convolutional layer ID. The new block should be inserted after the block. new_layer : An instance of StubLayer subclasses.","title":"Args"},{"location":"temp/graph/#to_wider_model","text":"Widen the last dimension of the output of the pre_layer.","title":"to_wider_model"},{"location":"temp/graph/#args_6","text":"pre_layer_id : The ID of a convolutional layer or dense layer. n_add : The number of dimensions to add.","title":"Args"},{"location":"temp/graph/#_insert_new_layers","text":"Insert the new_layers after the node with start_node_id.","title":"_insert_new_layers"},{"location":"temp/graph/#to_add_skip_model","text":"Add a weighted add skip-connection from after start node to end node.","title":"to_add_skip_model"},{"location":"temp/graph/#args_7","text":"start_id : The convolutional layer ID, after which to start the skip-connection. end_id : The convolutional layer ID, after which to end the skip-connection.","title":"Args"},{"location":"temp/graph/#to_concat_skip_model","text":"Add a weighted add concatenate connection from after start node to end node.","title":"to_concat_skip_model"},{"location":"temp/graph/#args_8","text":"start_id : The convolutional layer ID, after which to start the skip-connection. end_id : The convolutional layer ID, after which to end the skip-connection.","title":"Args"},{"location":"temp/graph/#extract_descriptor","text":"Extract the the description of the Graph as an instance of NetworkDescriptor.","title":"extract_descriptor"},{"location":"temp/graph/#produce_model","text":"Build a new torch model based on the current graph.","title":"produce_model"},{"location":"temp/graph/#produce_keras_model","text":"Build a new keras model based on the current graph.","title":"produce_keras_model"},{"location":"temp/graph/#get_main_chain_layers","text":"Return a list of layer IDs in the main chain.","title":"get_main_chain_layers"},{"location":"temp/graph/#get_main_chain","text":"Returns the main chain node ID list.","title":"get_main_chain"},{"location":"temp/graph/#torchmodel","text":"A neural network class using pytorch constructed from an instance of Graph.","title":"TorchModel"},{"location":"temp/image_supervised/","text":"_image_to_array Read the image from the path and return image object. Return an image object. Args img_path : image file name in images_dir_path. read_images Read the images from the path and return their numpy.ndarray instance. Return a numpy.ndarray instance containing the training data. Args img_file_names : List containing images names. images_dir_path : Path to the directory containing images. parallel : (Default load_image_dataset Load images from the files and labels from a csv file. Second, the dataset is a set of images and the labels are in a CSV file. The CSV file should contain two columns whose names are 'File Name' and 'Label'. The file names in the first column should match the file names of the images with extensions, e.g., .jpg, .png. The path to the CSV file should be passed through the csv_file_path . The path to the directory containing all the images should be passed through image_path . Args csv_file_path : CSV file path. images_path : Path where images exist. parallel : (Default Returns x : Four dimensional numpy.ndarray. The channel dimension is the last dimension. y : The labels. ImageSupervised Abstract image supervised class. Attributes path : A path to the directory to save the classifier as well as intermediate results. cnn : CNN module from net_module.py. y_encoder : Label encoder, used in transform_y or inverse_transform_y for encode the label. For example, if one hot encoder needed, y_encoder can be OneHotEncoder. data_transformer : A transformer class to process the data. See example as ImageDataTransformer. verbose : A boolean value indicating the verbosity mode which determines whether the search process will be printed to stdout. augment : A boolean value indicating whether the data needs augmentation. If not define, then it will use the value of Constant.DATA_AUGMENTATION which is True by default. searcher_args : A dictionary containing the parameters for the searcher's init function. resize_height : resize image height. resize_width : resize image width. init Initialize the instance. The classifier will be loaded from the files in 'path' if parameter 'resume' is True. Otherwise it would create a new one. Args: verbose: A boolean of whether the search process will be printed to stdout. path: A string. The path to a directory, where the intermediate results are saved. resume: A boolean. If True, the classifier will continue to previous work saved in path. Otherwise, the classifier will start a new search. searcher_args: A dictionary containing the parameters for the searcher's init function. augment: A boolean value indicating whether the data needs augmentation. If not define, then it will use the value of Constant.DATA_AUGMENTATION which is True by default. ImageClassifier ImageClassifier class. It is used for image classification. It searches convolutional neural network architectures for the best configuration for the image dataset. export_autokeras_model Creates and Exports the AutoKeras model to the given filename. ImageClassifier1D ImageClassifier1D class. It is used for 1D image classification. It searches convolutional neural network architectures for the best configuration for the 1D image dataset. ImageClassifier3D ImageClassifier3D class. It is used for 3D image classification. It searches convolutional neural network architectures for the best configuration for the 1D image dataset. ImageRegressor ImageRegressor class. It is used for image regression. It searches convolutional neural network architectures for the best configuration for the image dataset. export_autokeras_model Creates and Exports the AutoKeras model to the given filename. ImageRegressor1D ImageRegressor1D class. It is used for 1D image regression. It searches convolutional neural network architectures for the best configuration for the 1D image dataset. ImageRegressor3D ImageRegressor3D class. It is used for 3D image regression. It searches convolutional neural network architectures for the best configuration for the 1D image dataset. PortableImageSupervised init Initialize the instance. Args: graph: The graph form of the learned model","title":"image_supervised"},{"location":"temp/image_supervised/#_image_to_array","text":"Read the image from the path and return image object. Return an image object.","title":"_image_to_array"},{"location":"temp/image_supervised/#args","text":"img_path : image file name in images_dir_path.","title":"Args"},{"location":"temp/image_supervised/#read_images","text":"Read the images from the path and return their numpy.ndarray instance. Return a numpy.ndarray instance containing the training data.","title":"read_images"},{"location":"temp/image_supervised/#args_1","text":"img_file_names : List containing images names. images_dir_path : Path to the directory containing images. parallel : (Default","title":"Args"},{"location":"temp/image_supervised/#load_image_dataset","text":"Load images from the files and labels from a csv file. Second, the dataset is a set of images and the labels are in a CSV file. The CSV file should contain two columns whose names are 'File Name' and 'Label'. The file names in the first column should match the file names of the images with extensions, e.g., .jpg, .png. The path to the CSV file should be passed through the csv_file_path . The path to the directory containing all the images should be passed through image_path .","title":"load_image_dataset"},{"location":"temp/image_supervised/#args_2","text":"csv_file_path : CSV file path. images_path : Path where images exist. parallel : (Default","title":"Args"},{"location":"temp/image_supervised/#returns","text":"x : Four dimensional numpy.ndarray. The channel dimension is the last dimension. y : The labels.","title":"Returns"},{"location":"temp/image_supervised/#imagesupervised","text":"Abstract image supervised class.","title":"ImageSupervised"},{"location":"temp/image_supervised/#attributes","text":"path : A path to the directory to save the classifier as well as intermediate results. cnn : CNN module from net_module.py. y_encoder : Label encoder, used in transform_y or inverse_transform_y for encode the label. For example, if one hot encoder needed, y_encoder can be OneHotEncoder. data_transformer : A transformer class to process the data. See example as ImageDataTransformer. verbose : A boolean value indicating the verbosity mode which determines whether the search process will be printed to stdout. augment : A boolean value indicating whether the data needs augmentation. If not define, then it will use the value of Constant.DATA_AUGMENTATION which is True by default. searcher_args : A dictionary containing the parameters for the searcher's init function. resize_height : resize image height. resize_width : resize image width.","title":"Attributes"},{"location":"temp/image_supervised/#init","text":"Initialize the instance. The classifier will be loaded from the files in 'path' if parameter 'resume' is True. Otherwise it would create a new one. Args: verbose: A boolean of whether the search process will be printed to stdout. path: A string. The path to a directory, where the intermediate results are saved. resume: A boolean. If True, the classifier will continue to previous work saved in path. Otherwise, the classifier will start a new search. searcher_args: A dictionary containing the parameters for the searcher's init function. augment: A boolean value indicating whether the data needs augmentation. If not define, then it will use the value of Constant.DATA_AUGMENTATION which is True by default.","title":"init"},{"location":"temp/image_supervised/#imageclassifier","text":"ImageClassifier class. It is used for image classification. It searches convolutional neural network architectures for the best configuration for the image dataset.","title":"ImageClassifier"},{"location":"temp/image_supervised/#export_autokeras_model","text":"Creates and Exports the AutoKeras model to the given filename.","title":"export_autokeras_model"},{"location":"temp/image_supervised/#imageclassifier1d","text":"ImageClassifier1D class. It is used for 1D image classification. It searches convolutional neural network architectures for the best configuration for the 1D image dataset.","title":"ImageClassifier1D"},{"location":"temp/image_supervised/#imageclassifier3d","text":"ImageClassifier3D class. It is used for 3D image classification. It searches convolutional neural network architectures for the best configuration for the 1D image dataset.","title":"ImageClassifier3D"},{"location":"temp/image_supervised/#imageregressor","text":"ImageRegressor class. It is used for image regression. It searches convolutional neural network architectures for the best configuration for the image dataset.","title":"ImageRegressor"},{"location":"temp/image_supervised/#export_autokeras_model_1","text":"Creates and Exports the AutoKeras model to the given filename.","title":"export_autokeras_model"},{"location":"temp/image_supervised/#imageregressor1d","text":"ImageRegressor1D class. It is used for 1D image regression. It searches convolutional neural network architectures for the best configuration for the 1D image dataset.","title":"ImageRegressor1D"},{"location":"temp/image_supervised/#imageregressor3d","text":"ImageRegressor3D class. It is used for 3D image regression. It searches convolutional neural network architectures for the best configuration for the 1D image dataset.","title":"ImageRegressor3D"},{"location":"temp/image_supervised/#portableimagesupervised","text":"","title":"PortableImageSupervised"},{"location":"temp/image_supervised/#init_1","text":"Initialize the instance. Args: graph: The graph form of the learned model","title":"init"},{"location":"temp/layer_transformer/","text":"","title":"Layer transformer"},{"location":"temp/layers/","text":"","title":"Layers"},{"location":"temp/loss_function/","text":"","title":"Loss function"},{"location":"temp/metric/","text":"","title":"Metric"},{"location":"temp/model/","text":"MultiSpeakerTTSModel Attention seq2seq model + post processing network AttentionSeq2Seq Encoder + Decoder with attention","title":"Model"},{"location":"temp/model/#multispeakerttsmodel","text":"Attention seq2seq model + post processing network","title":"MultiSpeakerTTSModel"},{"location":"temp/model/#attentionseq2seq","text":"Encoder + Decoder with attention","title":"AttentionSeq2Seq"},{"location":"temp/model_trainer/","text":"ModelTrainerBase A base class all model trainers will inherit from. Attributes device : A string. Indicating the device to use. 'cuda' or 'cpu'. train_loader : Training data wrapped in batches in Pytorch Dataloader. test_loader : Testing data wrapped in batches in Pytorch Dataloader. loss_function : A function with two parameters (prediction, target). There is no specific requirement for the types of the parameters, as long as they are compatible with the model and the data loaders. The prediction should be the output of the model for a batch. The target should be a batch of targets packed in the data loaders. metric : It should be a subclass of class autokeras.metric.Metric. In the compute(prediction, target) function, prediction and targets are, all numpy arrays converted from the output of the model and the targets packed in the data loaders. verbose : Verbosity mode. train_model Train the model. Args timeout : timeout in seconds max_iter_num : int, maximum numer of iteration max_no_improvement_num : after max_no_improvement_num, if the model still makes no improvement, finish training. ModelTrainer A class that is used to train the model. This class can train a Pytorch model with the given data loaders. The metric, loss_function, and model must be compatible with each other. Please see the details in the Attributes. Attributes temp_model_path : Specify the path where temp model should be stored. model : An instance of Pytorch Module. The model that will be trained. early_stop : An instance of class EarlyStop. optimizer : The optimizer is chosen to use the Pytorch Adam optimizer. current_epoch : Record the current epoch. train_model Train the model. Train the model with max_iter_num or max_no_improvement_num is met. Args timeout : timeout in seconds max_iter_num : An integer. The maximum number of epochs to train the model. The training will stop when this number is reached. max_no_improvement_num : An integer. The maximum number of epochs when the loss value doesn't decrease. The training will stop when this number is reached. Returns _train Where the actual train proceed. _test Function for evaluation. GANModelTrainer A ModelTrainer especially for the GAN. Attributes d_model : A discriminator model. g_model : A generator model. out_f : Out file. out_size : Size of the output image. optimizer_d : Optimizer for discriminator. optimizer_g : Optimizer for generator. init Initialize the GANModelTrainer. Args g_model : The generator model to be trained. d_model : The discriminator model to be trained. train_data : the training data. loss_function : The loss function for both discriminator and generator. verbose : Whether to output the system output. gen_training_result : Whether to generate the intermediate result while training. _train Perform the actual train. EarlyStop A class check for early stop condition. Attributes training_losses : Record all the training loss. minimum_loss : The minimum loss we achieve so far. Used to compared to determine no improvement condition. no_improvement_count : Current no improvement count. _max_no_improvement_num : The maximum number specified. _done : Whether condition met. _min_loss_dec : A threshold for loss improvement. on_train_begin Initiate the early stop condition. Call on every time the training iteration begins. on_epoch_end Check the early stop condition. Call on every time the training iteration end. Args loss : The loss function achieved by the epoch. Returns","title":"model_trainer"},{"location":"temp/model_trainer/#modeltrainerbase","text":"A base class all model trainers will inherit from.","title":"ModelTrainerBase"},{"location":"temp/model_trainer/#attributes","text":"device : A string. Indicating the device to use. 'cuda' or 'cpu'. train_loader : Training data wrapped in batches in Pytorch Dataloader. test_loader : Testing data wrapped in batches in Pytorch Dataloader. loss_function : A function with two parameters (prediction, target). There is no specific requirement for the types of the parameters, as long as they are compatible with the model and the data loaders. The prediction should be the output of the model for a batch. The target should be a batch of targets packed in the data loaders. metric : It should be a subclass of class autokeras.metric.Metric. In the compute(prediction, target) function, prediction and targets are, all numpy arrays converted from the output of the model and the targets packed in the data loaders. verbose : Verbosity mode.","title":"Attributes"},{"location":"temp/model_trainer/#train_model","text":"Train the model.","title":"train_model"},{"location":"temp/model_trainer/#args","text":"timeout : timeout in seconds max_iter_num : int, maximum numer of iteration max_no_improvement_num : after max_no_improvement_num, if the model still makes no improvement, finish training.","title":"Args"},{"location":"temp/model_trainer/#modeltrainer","text":"A class that is used to train the model. This class can train a Pytorch model with the given data loaders. The metric, loss_function, and model must be compatible with each other. Please see the details in the Attributes.","title":"ModelTrainer"},{"location":"temp/model_trainer/#attributes_1","text":"temp_model_path : Specify the path where temp model should be stored. model : An instance of Pytorch Module. The model that will be trained. early_stop : An instance of class EarlyStop. optimizer : The optimizer is chosen to use the Pytorch Adam optimizer. current_epoch : Record the current epoch.","title":"Attributes"},{"location":"temp/model_trainer/#train_model_1","text":"Train the model. Train the model with max_iter_num or max_no_improvement_num is met.","title":"train_model"},{"location":"temp/model_trainer/#args_1","text":"timeout : timeout in seconds max_iter_num : An integer. The maximum number of epochs to train the model. The training will stop when this number is reached. max_no_improvement_num : An integer. The maximum number of epochs when the loss value doesn't decrease. The training will stop when this number is reached.","title":"Args"},{"location":"temp/model_trainer/#returns","text":"","title":"Returns"},{"location":"temp/model_trainer/#_train","text":"Where the actual train proceed.","title":"_train"},{"location":"temp/model_trainer/#_test","text":"Function for evaluation.","title":"_test"},{"location":"temp/model_trainer/#ganmodeltrainer","text":"A ModelTrainer especially for the GAN.","title":"GANModelTrainer"},{"location":"temp/model_trainer/#attributes_2","text":"d_model : A discriminator model. g_model : A generator model. out_f : Out file. out_size : Size of the output image. optimizer_d : Optimizer for discriminator. optimizer_g : Optimizer for generator.","title":"Attributes"},{"location":"temp/model_trainer/#init","text":"Initialize the GANModelTrainer.","title":"init"},{"location":"temp/model_trainer/#args_2","text":"g_model : The generator model to be trained. d_model : The discriminator model to be trained. train_data : the training data. loss_function : The loss function for both discriminator and generator. verbose : Whether to output the system output. gen_training_result : Whether to generate the intermediate result while training.","title":"Args"},{"location":"temp/model_trainer/#_train_1","text":"Perform the actual train.","title":"_train"},{"location":"temp/model_trainer/#earlystop","text":"A class check for early stop condition.","title":"EarlyStop"},{"location":"temp/model_trainer/#attributes_3","text":"training_losses : Record all the training loss. minimum_loss : The minimum loss we achieve so far. Used to compared to determine no improvement condition. no_improvement_count : Current no improvement count. _max_no_improvement_num : The maximum number specified. _done : Whether condition met. _min_loss_dec : A threshold for loss improvement.","title":"Attributes"},{"location":"temp/model_trainer/#on_train_begin","text":"Initiate the early stop condition. Call on every time the training iteration begins.","title":"on_train_begin"},{"location":"temp/model_trainer/#on_epoch_end","text":"Check the early stop condition. Call on every time the training iteration end.","title":"on_epoch_end"},{"location":"temp/model_trainer/#args_3","text":"loss : The loss function achieved by the epoch.","title":"Args"},{"location":"temp/model_trainer/#returns_1","text":"","title":"Returns"},{"location":"temp/modeling/","text":"gelu Implementation of the gelu activation function. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results): 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) BertConfig Configuration class to store the configuration of a BertModel . init Constructs BertConfig. Args vocab_size_or_config_json_file : Vocabulary size of inputs_ids in BertModel . hidden_size : Size of the encoder layers and the pooler layer. num_hidden_layers : Number of hidden layers in the Transformer encoder. num_attention_heads : Number of attention heads for each attention layer in the Transformer encoder. intermediate_size : The size of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder. hidden_act : The non-linear activation function (function or string) in the encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported. hidden_dropout_prob : The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler. attention_probs_dropout_prob : The dropout ratio for the attention probabilities. max_position_embeddings : The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048). type_vocab_size : The vocabulary size of the token_type_ids passed into BertModel . initializer_range : The sttdev of the truncated_normal_initializer for initializing all weight matrices. from_dict Constructs a BertConfig from a Python dictionary of parameters. from_json_file Constructs a BertConfig from a json file of parameters. to_dict Serializes this instance to a Python dictionary. to_json_string Serializes this instance to a JSON string. BertEmbeddings Construct the embeddings from word, position and token_type embeddings. PreTrainedBertModel An abstract class to handle weights initialization and a simple interface for dowloading and loading pretrained models. init_bert_weights Initialize the weights. from_pretrained Instantiate a PreTrainedBertModel from a pre-trained model file or a pytorch state dict. Download and cache the pre-trained model file if needed. Params: pretrained_model_name: either: - a str with the name of a pre-trained model to load selected in the list of: . bert-base-uncased . bert-large-uncased . bert-base-cased . bert-base-multilingual . bert-base-chinese - a path or url to a pretrained model archive containing: . bert_config.json a configuration file for the model . pytorch_model.bin a PyTorch dump of a BertForPreTraining instance cache_dir: an optional path to a folder in which the pre-trained models will be cached. state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models inputs, *kwargs: additional input for the specific Bert class (ex: num_labels for BertForSequenceClassification) BertModel BERT model (\"Bidirectional Embedding Representations from a Transformer\"). Params: config: a BertConfig class instance with the configuration to build a new model Inputs: input_ids : a torch.LongTensor of shape [batch_size, sequence_length] with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts extract_features.py , run_classifier.py and run_squad.py ) token_type_ids : an optional torch.LongTensor of shape [batch_size, sequence_length] with the token types indices selected in [0, 1]. Type 0 corresponds to a sentence A and type 1 corresponds to a sentence B token (see BERT paper for more details). attention_mask : an optional torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max input sequence length in the current batch. It's the mask that we typically use for attention when a batch has varying length sentences. output_all_encoded_layers : boolean which controls the content of the encoded_layers output as described below. Default: True . Outputs: Tuple of (encoded_layers, pooled_output) encoded_layers : controled by output_all_encoded_layers argument: - output_all_encoded_layers=True : outputs a list of the full sequences of encoded-hidden-states at the end of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size], - output_all_encoded_layers=False : outputs only the full sequence of hidden-states corresponding to the last attention block of shape [batch_size, sequence_length, hidden_size], pooled_output : a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a classifier pretrained on top of the hidden state associated to the first character of the input ( CLF ) to train on the Next-Sentence task (see BERT's paper). Example usage: python # Already been converted into WordPiece token ids input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]]) input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]]) token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]]) config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072) model = modeling.BertModel(config=config) all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask) BertForSequenceClassification BERT model for classification. This module is composed of the BERT model with a linear layer on top of the pooled output. Params: config : a BertConfig class instance with the configuration to build a new model. num_labels : the number of classes for the classifier. Default = 2. Inputs: input_ids : a torch.LongTensor of shape [batch_size, sequence_length] with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts extract_features.py , run_classifier.py and run_squad.py ) token_type_ids : an optional torch.LongTensor of shape [batch_size, sequence_length] with the token types indices selected in [0, 1]. Type 0 corresponds to a sentence A and type 1 corresponds to a sentence B token (see BERT paper for more details). attention_mask : an optional torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max input sequence length in the current batch. It's the mask that we typically use for attention when a batch has varying length sentences. labels : labels for the classification output: torch.LongTensor of shape [batch_size] with indices selected in [0, ..., num_labels]. Outputs: if labels is not None : Outputs the CrossEntropy classification loss of the output with the labels. if labels is None : Outputs the classification logits of shape [batch_size, num_labels]. Example usage: python # Already been converted into WordPiece token ids input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]]) input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]]) token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]]) config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072) num_labels = 2 model = BertForSequenceClassification(config, num_labels) logits = model(input_ids, token_type_ids, input_mask)","title":"Modeling"},{"location":"temp/modeling/#gelu","text":"Implementation of the gelu activation function. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results): 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))","title":"gelu"},{"location":"temp/modeling/#bertconfig","text":"Configuration class to store the configuration of a BertModel .","title":"BertConfig"},{"location":"temp/modeling/#init","text":"Constructs BertConfig.","title":"init"},{"location":"temp/modeling/#args","text":"vocab_size_or_config_json_file : Vocabulary size of inputs_ids in BertModel . hidden_size : Size of the encoder layers and the pooler layer. num_hidden_layers : Number of hidden layers in the Transformer encoder. num_attention_heads : Number of attention heads for each attention layer in the Transformer encoder. intermediate_size : The size of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder. hidden_act : The non-linear activation function (function or string) in the encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported. hidden_dropout_prob : The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler. attention_probs_dropout_prob : The dropout ratio for the attention probabilities. max_position_embeddings : The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048). type_vocab_size : The vocabulary size of the token_type_ids passed into BertModel . initializer_range : The sttdev of the truncated_normal_initializer for initializing all weight matrices.","title":"Args"},{"location":"temp/modeling/#from_dict","text":"Constructs a BertConfig from a Python dictionary of parameters.","title":"from_dict"},{"location":"temp/modeling/#from_json_file","text":"Constructs a BertConfig from a json file of parameters.","title":"from_json_file"},{"location":"temp/modeling/#to_dict","text":"Serializes this instance to a Python dictionary.","title":"to_dict"},{"location":"temp/modeling/#to_json_string","text":"Serializes this instance to a JSON string.","title":"to_json_string"},{"location":"temp/modeling/#bertembeddings","text":"Construct the embeddings from word, position and token_type embeddings.","title":"BertEmbeddings"},{"location":"temp/modeling/#pretrainedbertmodel","text":"An abstract class to handle weights initialization and a simple interface for dowloading and loading pretrained models.","title":"PreTrainedBertModel"},{"location":"temp/modeling/#init_bert_weights","text":"Initialize the weights.","title":"init_bert_weights"},{"location":"temp/modeling/#from_pretrained","text":"Instantiate a PreTrainedBertModel from a pre-trained model file or a pytorch state dict. Download and cache the pre-trained model file if needed. Params: pretrained_model_name: either: - a str with the name of a pre-trained model to load selected in the list of: . bert-base-uncased . bert-large-uncased . bert-base-cased . bert-base-multilingual . bert-base-chinese - a path or url to a pretrained model archive containing: . bert_config.json a configuration file for the model . pytorch_model.bin a PyTorch dump of a BertForPreTraining instance cache_dir: an optional path to a folder in which the pre-trained models will be cached. state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models inputs, *kwargs: additional input for the specific Bert class (ex: num_labels for BertForSequenceClassification)","title":"from_pretrained"},{"location":"temp/modeling/#bertmodel","text":"BERT model (\"Bidirectional Embedding Representations from a Transformer\"). Params: config: a BertConfig class instance with the configuration to build a new model Inputs: input_ids : a torch.LongTensor of shape [batch_size, sequence_length] with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts extract_features.py , run_classifier.py and run_squad.py ) token_type_ids : an optional torch.LongTensor of shape [batch_size, sequence_length] with the token types indices selected in [0, 1]. Type 0 corresponds to a sentence A and type 1 corresponds to a sentence B token (see BERT paper for more details). attention_mask : an optional torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max input sequence length in the current batch. It's the mask that we typically use for attention when a batch has varying length sentences. output_all_encoded_layers : boolean which controls the content of the encoded_layers output as described below. Default: True . Outputs: Tuple of (encoded_layers, pooled_output) encoded_layers : controled by output_all_encoded_layers argument: - output_all_encoded_layers=True : outputs a list of the full sequences of encoded-hidden-states at the end of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size], - output_all_encoded_layers=False : outputs only the full sequence of hidden-states corresponding to the last attention block of shape [batch_size, sequence_length, hidden_size], pooled_output : a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a classifier pretrained on top of the hidden state associated to the first character of the input ( CLF ) to train on the Next-Sentence task (see BERT's paper). Example usage: python # Already been converted into WordPiece token ids input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]]) input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]]) token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]]) config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072) model = modeling.BertModel(config=config) all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)","title":"BertModel"},{"location":"temp/modeling/#bertforsequenceclassification","text":"BERT model for classification. This module is composed of the BERT model with a linear layer on top of the pooled output. Params: config : a BertConfig class instance with the configuration to build a new model. num_labels : the number of classes for the classifier. Default = 2. Inputs: input_ids : a torch.LongTensor of shape [batch_size, sequence_length] with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts extract_features.py , run_classifier.py and run_squad.py ) token_type_ids : an optional torch.LongTensor of shape [batch_size, sequence_length] with the token types indices selected in [0, 1]. Type 0 corresponds to a sentence A and type 1 corresponds to a sentence B token (see BERT paper for more details). attention_mask : an optional torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max input sequence length in the current batch. It's the mask that we typically use for attention when a batch has varying length sentences. labels : labels for the classification output: torch.LongTensor of shape [batch_size] with indices selected in [0, ..., num_labels]. Outputs: if labels is not None : Outputs the CrossEntropy classification loss of the output with the labels. if labels is None : Outputs the classification logits of shape [batch_size, num_labels]. Example usage: python # Already been converted into WordPiece token ids input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]]) input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]]) token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]]) config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072) num_labels = 2 model = BertForSequenceClassification(config, num_labels) logits = model(input_ids, token_type_ids, input_mask)","title":"BertForSequenceClassification"},{"location":"temp/modules/","text":"position_encoding_init Init the sinusoid position encoding table linear Weight-normalized Linear layer (input: N x T x C) Conv1dGLU (Dilated) Conv1d + Gated linear unit + (optionally) speaker embedding","title":"Modules"},{"location":"temp/modules/#position_encoding_init","text":"Init the sinusoid position encoding table","title":"position_encoding_init"},{"location":"temp/modules/#linear","text":"Weight-normalized Linear layer (input: N x T x C)","title":"linear"},{"location":"temp/modules/#conv1dglu","text":"(Dilated) Conv1d + Gated linear unit + (optionally) speaker embedding","title":"Conv1dGLU"},{"location":"temp/nas/","text":"Neural Architecture Search To help the researchers to do experiments on neural architecture search (NAS), we have implemented several baseline methods using the Auto-Keras framework. The implementations are easy since only the core part of the search algorithm is needed. All other parts of NAS (e.g. data structures for storing neural architectures, training of the neural networks) are done by the Auto-Keras framework. Why implement NAS papers in Auto-Keras? The NAS papers usually evaluate their work with the same dataset (e.g. CIFAR10), but they are not directly comparable because of the data preparation and training process are different, the influence of which are significant enough to change the rankings of these NAS methods. We have implemented some of the NAS methods in the framework. More state-of-the-art methods are in progress. There are three advantages of implementing the NAS methods in Auto-Keras. First, it fairly compares the NAS methods independent from other factors (e.g. the choice of optimizer, data augmentation). Second, researchers can easily change the experiment datasets used for NAS. Many of the currently available NAS implementations couple too much with the dataset used, which makes it hard to replace the original dataset with a new one. Third, it saves the effort of finding and running code from different sources. Different code may have different requirements of dependencies and environments, which may conflict with each other. Baseline methods implemented Description of each baseline method. How to run the baseline methods? Code example containing two parts: data preparation, search. Should call CnnModule. How to implement your own search? You are welcome to implement your own method for NAS in our framework. If it works well, we are happy to merge it into our repo.","title":"Neural Architecture Search"},{"location":"temp/nas/#neural-architecture-search","text":"To help the researchers to do experiments on neural architecture search (NAS), we have implemented several baseline methods using the Auto-Keras framework. The implementations are easy since only the core part of the search algorithm is needed. All other parts of NAS (e.g. data structures for storing neural architectures, training of the neural networks) are done by the Auto-Keras framework.","title":"Neural Architecture Search"},{"location":"temp/nas/#why-implement-nas-papers-in-auto-keras","text":"The NAS papers usually evaluate their work with the same dataset (e.g. CIFAR10), but they are not directly comparable because of the data preparation and training process are different, the influence of which are significant enough to change the rankings of these NAS methods. We have implemented some of the NAS methods in the framework. More state-of-the-art methods are in progress. There are three advantages of implementing the NAS methods in Auto-Keras. First, it fairly compares the NAS methods independent from other factors (e.g. the choice of optimizer, data augmentation). Second, researchers can easily change the experiment datasets used for NAS. Many of the currently available NAS implementations couple too much with the dataset used, which makes it hard to replace the original dataset with a new one. Third, it saves the effort of finding and running code from different sources. Different code may have different requirements of dependencies and environments, which may conflict with each other.","title":"Why implement NAS papers in Auto-Keras?"},{"location":"temp/nas/#baseline-methods-implemented","text":"Description of each baseline method.","title":"Baseline methods implemented"},{"location":"temp/nas/#how-to-run-the-baseline-methods","text":"Code example containing two parts: data preparation, search. Should call CnnModule.","title":"How to run the baseline methods?"},{"location":"temp/nas/#how-to-implement-your-own-search","text":"You are welcome to implement your own method for NAS in our framework. If it works well, we are happy to merge it into our repo.","title":"How to implement your own search?"},{"location":"temp/net_module/","text":"NetworkModule Class to create a network module. Attributes loss : A function taking two parameters, the predictions and the ground truth. metric : An instance of the Metric subclasses. searcher_args : A dictionary containing the parameters for the searcher's init function. searcher : An instance of the Searcher class. path : A string. The path to the directory to save the searcher. verbose : A boolean. Setting it to true prints to stdout. generators : A list of instances of the NetworkGenerator class or its subclasses. search_type : A constant denoting the type of hyperparameter search algorithm that must be used. fit Search the best network. Args n_output_node : A integer value represent the number of output node in the final layer. input_shape : A tuple to express the shape of every train entry. For example, MNIST dataset would be (28,28,1). train_data : A PyTorch DataLoader instance representing the training data. test_data : A PyTorch DataLoader instance representing the testing data. time_limit : A integer value represents the time limit on searching for models. final_fit Final training after found the best architecture. Args train_data : A DataLoader instance representing the training data. test_data : A DataLoader instance representing the testing data. trainer_args : A dictionary containing the parameters of the ModelTrainer constructor. retrain : A boolean of whether reinitialize the weights of the model. CnnModule Class to create a CNN module. MlpModule Class to create an MLP module.","title":"Net module"},{"location":"temp/net_module/#networkmodule","text":"Class to create a network module.","title":"NetworkModule"},{"location":"temp/net_module/#attributes","text":"loss : A function taking two parameters, the predictions and the ground truth. metric : An instance of the Metric subclasses. searcher_args : A dictionary containing the parameters for the searcher's init function. searcher : An instance of the Searcher class. path : A string. The path to the directory to save the searcher. verbose : A boolean. Setting it to true prints to stdout. generators : A list of instances of the NetworkGenerator class or its subclasses. search_type : A constant denoting the type of hyperparameter search algorithm that must be used.","title":"Attributes"},{"location":"temp/net_module/#fit","text":"Search the best network.","title":"fit"},{"location":"temp/net_module/#args","text":"n_output_node : A integer value represent the number of output node in the final layer. input_shape : A tuple to express the shape of every train entry. For example, MNIST dataset would be (28,28,1). train_data : A PyTorch DataLoader instance representing the training data. test_data : A PyTorch DataLoader instance representing the testing data. time_limit : A integer value represents the time limit on searching for models.","title":"Args"},{"location":"temp/net_module/#final_fit","text":"Final training after found the best architecture.","title":"final_fit"},{"location":"temp/net_module/#args_1","text":"train_data : A DataLoader instance representing the training data. test_data : A DataLoader instance representing the testing data. trainer_args : A dictionary containing the parameters of the ModelTrainer constructor. retrain : A boolean of whether reinitialize the weights of the model.","title":"Args"},{"location":"temp/net_module/#cnnmodule","text":"Class to create a CNN module.","title":"CnnModule"},{"location":"temp/net_module/#mlpmodule","text":"Class to create an MLP module.","title":"MlpModule"},{"location":"temp/net_transformer/","text":"","title":"Net transformer"},{"location":"temp/numbers/","text":"","title":"Numbers"},{"location":"temp/object_detector/","text":"decode Decode locations from predictions using priors to undo the encoding we did for offset regression at train time. Args: loc (tensor): location predictions for loc layers, Shape: [num_priors,4] priors (tensor): Prior boxes in center-offset form. Shape: [num_priors,4]. variances: (list[float]) Variances of priorboxes Return: decoded bounding box predictions nms Apply non-maximum suppression at test time to avoid detecting too many overlapping bounding boxes for a given object. Args: boxes: (tensor) The location preds for the img, Shape: [num_priors,4]. scores: (tensor) The class predscores for the img, Shape:[num_priors]. overlap: (float) The overlap thresh for suppressing unnecessary boxes. top_k: (int) The Maximum number of box preds to consider. Return: The indices of the kept boxes with respect to num_priors. SSD Single Shot Multibox Architecture The network is composed of a base VGG network followed by the added multibox conv layers. Each multibox layer branches into 1) conv2d for class conf scores 2) conv2d for localization predictions 3) associated priorbox layer to produce default bounding boxes specific to the layer's feature map size. See: https://arxiv.org/pdf/1512.02325.pdf for more details. Args phase : (string) Can be \"test\" or \"train\" size : input image size base : VGG16 layers for input, size of either 300 or 500 extras : extra layers that feed to multibox loc and conf layers head : \"multibox head\" consists of loc and conf conv layers forward Applies network layers and ops on input image(s) x. Args x : input image or batch of images. Shape rn : Depending on phase : test : Variable(tensor) of output class label predictions, confidence score, and corresponding location predictions for each object detected. Shape train : list of concat outputs from PriorBox Compute priorbox coordinates in center-offset form for each source feature map. Detect At test time, Detect is the final layer of SSD. Decode location preds, apply non-maximum suppression to location predictions based on conf scores and threshold to a top_k number of output predictions for both confidence score and locations. forward Args: loc_data: (tensor) Loc preds from loc layers Shape: [batch,num_priors 4] conf_data: (tensor) Shape: Conf preds from conf layers Shape: [batch num_priors,num_classes] prior_data: (tensor) Prior boxes and variances from priorbox layers Shape: [1,num_priors,4] ObjectDetector predict Returns: List of dictionaries. Each dictionary is like {\"left\": int, \"top\": int, \"width\": int, \"height\": int: \"category\": str, \"confidence\": float}","title":"Object detector"},{"location":"temp/object_detector/#decode","text":"Decode locations from predictions using priors to undo the encoding we did for offset regression at train time. Args: loc (tensor): location predictions for loc layers, Shape: [num_priors,4] priors (tensor): Prior boxes in center-offset form. Shape: [num_priors,4]. variances: (list[float]) Variances of priorboxes Return: decoded bounding box predictions","title":"decode"},{"location":"temp/object_detector/#nms","text":"Apply non-maximum suppression at test time to avoid detecting too many overlapping bounding boxes for a given object. Args: boxes: (tensor) The location preds for the img, Shape: [num_priors,4]. scores: (tensor) The class predscores for the img, Shape:[num_priors]. overlap: (float) The overlap thresh for suppressing unnecessary boxes. top_k: (int) The Maximum number of box preds to consider. Return: The indices of the kept boxes with respect to num_priors.","title":"nms"},{"location":"temp/object_detector/#ssd","text":"Single Shot Multibox Architecture The network is composed of a base VGG network followed by the added multibox conv layers. Each multibox layer branches into 1) conv2d for class conf scores 2) conv2d for localization predictions 3) associated priorbox layer to produce default bounding boxes specific to the layer's feature map size. See: https://arxiv.org/pdf/1512.02325.pdf for more details.","title":"SSD"},{"location":"temp/object_detector/#args","text":"phase : (string) Can be \"test\" or \"train\" size : input image size base : VGG16 layers for input, size of either 300 or 500 extras : extra layers that feed to multibox loc and conf layers head : \"multibox head\" consists of loc and conf conv layers","title":"Args"},{"location":"temp/object_detector/#forward","text":"Applies network layers and ops on input image(s) x.","title":"forward"},{"location":"temp/object_detector/#args_1","text":"x : input image or batch of images. Shape rn : Depending on phase : test : Variable(tensor) of output class label predictions, confidence score, and corresponding location predictions for each object detected. Shape train : list of concat outputs from","title":"Args"},{"location":"temp/object_detector/#priorbox","text":"Compute priorbox coordinates in center-offset form for each source feature map.","title":"PriorBox"},{"location":"temp/object_detector/#detect","text":"At test time, Detect is the final layer of SSD. Decode location preds, apply non-maximum suppression to location predictions based on conf scores and threshold to a top_k number of output predictions for both confidence score and locations.","title":"Detect"},{"location":"temp/object_detector/#forward_1","text":"Args: loc_data: (tensor) Loc preds from loc layers Shape: [batch,num_priors 4] conf_data: (tensor) Shape: Conf preds from conf layers Shape: [batch num_priors,num_classes] prior_data: (tensor) Prior boxes and variances from priorbox layers Shape: [1,num_priors,4]","title":"forward"},{"location":"temp/object_detector/#objectdetector","text":"","title":"ObjectDetector"},{"location":"temp/object_detector/#predict","text":"Returns: List of dictionaries. Each dictionary is like {\"left\": int, \"top\": int, \"width\": int, \"height\": int: \"category\": str, \"confidence\": float}","title":"predict"},{"location":"temp/predefined_model/","text":"PredefinedModel The base class for the predefined model without architecture search Attributes graph : The graph form of the model. y_encoder : Label encoder, used in transform_y or inverse_transform_y for encode the label. For example, if one hot encoder needed, y_encoder can be OneHotEncoder. data_transformer : A instance of transformer to process the data, See example as ImageDataTransformer. verbose : A boolean of whether the search process will be printed to stdout. path : A string. The path to a directory, where the intermediate results are saved. _init_generator Initialize the generator to generate the model architecture. Args n_output_node : A integer value represent the number of output node in the final layer. input_shape : A tuple to express the shape of every train entry. fit Trains the model on the dataset given. Args x : A numpy.ndarray instance containing the training data or the training data combined with the validation data. y : A numpy.ndarray instance containing the label of the training data. or the label of the training data combined with the validation label. trainer_args : A dictionary containing the parameters of the ModelTrainer constructor.","title":"Predefined model"},{"location":"temp/predefined_model/#predefinedmodel","text":"The base class for the predefined model without architecture search","title":"PredefinedModel"},{"location":"temp/predefined_model/#attributes","text":"graph : The graph form of the model. y_encoder : Label encoder, used in transform_y or inverse_transform_y for encode the label. For example, if one hot encoder needed, y_encoder can be OneHotEncoder. data_transformer : A instance of transformer to process the data, See example as ImageDataTransformer. verbose : A boolean of whether the search process will be printed to stdout. path : A string. The path to a directory, where the intermediate results are saved.","title":"Attributes"},{"location":"temp/predefined_model/#_init_generator","text":"Initialize the generator to generate the model architecture.","title":"_init_generator"},{"location":"temp/predefined_model/#args","text":"n_output_node : A integer value represent the number of output node in the final layer. input_shape : A tuple to express the shape of every train entry.","title":"Args"},{"location":"temp/predefined_model/#fit","text":"Trains the model on the dataset given.","title":"fit"},{"location":"temp/predefined_model/#args_1","text":"x : A numpy.ndarray instance containing the training data or the training data combined with the validation data. y : A numpy.ndarray instance containing the label of the training data. or the label of the training data combined with the validation label. trainer_args : A dictionary containing the parameters of the ModelTrainer constructor.","title":"Args"},{"location":"temp/preprocessor/","text":"OneHotEncoder A class that can format data. This class provides ways to transform data's classification label into vector. Attributes data : The input data n_classes : The number of classes in the classification problem. labels : The number of labels. label_to_vec : Mapping from label to vector. int_to_label : Mapping from int to label. init Initialize a OneHotEncoder fit Create mapping from label to vector, and vector to label. transform Get vector for every element in the data array. inverse_transform Get label for every element in data. Cutout Randomly mask out one or more patches from an image. Args n_holes (int) : Number of patches to cut out of each image. length (int) : The length (in pixels) of each square patch. call Perform the actual transformation. Args img (Tensor) : Tensor image of size (C, H, W). Returns Tensor : Image with n_holes of dimension length x length cut out of it. DataTransformer A superclass for all the DataTransformer. transform_train Transform the training data and get the DataLoader class. Args data : x. targets : y. batch_size : the batch size. Returns dataloader : A torch.DataLoader class to represent the transformed data. transform_test Transform the training data and get the DataLoader class. Args data : x. targets : y. batch_size : the batch size. Returns dataloader : A torch.DataLoader class to represent the transformed data. TextDataTransformer A DataTransformer class for the text data. transform_train Transform the training dataset. transform_test Transform the testing dataset. ImageDataTransformer Perform basic image transformation and augmentation. Attributes max_val : the maximum value of all data. mean : the mean value. std : the standard deviation. augment : whether to perform augmentation on data. transform_train Transform the training data, perform random cropping data augmentation and basic random flip augmentation. Args data : Numpy array. The data to be transformed. batch_size : int batch_size. targets : the target of training set. Returns transform_test Transform the test data, perform normalization. Args data : Numpy array. The data to be transformed. batch_size : int batch_size. targets : the target of test set. Returns _transform Perform the actual transformation. Args compose_list : a list of transforming operation. data : x. targets : y. Returns MultiTransformDataset A class incorporate all transform method into a torch.Dataset class. BatchDataset A torch.Dataset class that can read data batch by batch.","title":"preprocessor"},{"location":"temp/preprocessor/#onehotencoder","text":"A class that can format data. This class provides ways to transform data's classification label into vector.","title":"OneHotEncoder"},{"location":"temp/preprocessor/#attributes","text":"data : The input data n_classes : The number of classes in the classification problem. labels : The number of labels. label_to_vec : Mapping from label to vector. int_to_label : Mapping from int to label.","title":"Attributes"},{"location":"temp/preprocessor/#init","text":"Initialize a OneHotEncoder","title":"init"},{"location":"temp/preprocessor/#fit","text":"Create mapping from label to vector, and vector to label.","title":"fit"},{"location":"temp/preprocessor/#transform","text":"Get vector for every element in the data array.","title":"transform"},{"location":"temp/preprocessor/#inverse_transform","text":"Get label for every element in data.","title":"inverse_transform"},{"location":"temp/preprocessor/#cutout","text":"Randomly mask out one or more patches from an image.","title":"Cutout"},{"location":"temp/preprocessor/#args","text":"n_holes (int) : Number of patches to cut out of each image. length (int) : The length (in pixels) of each square patch.","title":"Args"},{"location":"temp/preprocessor/#call","text":"Perform the actual transformation.","title":"call"},{"location":"temp/preprocessor/#args_1","text":"img (Tensor) : Tensor image of size (C, H, W).","title":"Args"},{"location":"temp/preprocessor/#returns","text":"Tensor : Image with n_holes of dimension length x length cut out of it.","title":"Returns"},{"location":"temp/preprocessor/#datatransformer","text":"A superclass for all the DataTransformer.","title":"DataTransformer"},{"location":"temp/preprocessor/#transform_train","text":"Transform the training data and get the DataLoader class.","title":"transform_train"},{"location":"temp/preprocessor/#args_2","text":"data : x. targets : y. batch_size : the batch size.","title":"Args"},{"location":"temp/preprocessor/#returns_1","text":"dataloader : A torch.DataLoader class to represent the transformed data.","title":"Returns"},{"location":"temp/preprocessor/#transform_test","text":"Transform the training data and get the DataLoader class.","title":"transform_test"},{"location":"temp/preprocessor/#args_3","text":"data : x. targets : y. batch_size : the batch size.","title":"Args"},{"location":"temp/preprocessor/#returns_2","text":"dataloader : A torch.DataLoader class to represent the transformed data.","title":"Returns"},{"location":"temp/preprocessor/#textdatatransformer","text":"A DataTransformer class for the text data.","title":"TextDataTransformer"},{"location":"temp/preprocessor/#transform_train_1","text":"Transform the training dataset.","title":"transform_train"},{"location":"temp/preprocessor/#transform_test_1","text":"Transform the testing dataset.","title":"transform_test"},{"location":"temp/preprocessor/#imagedatatransformer","text":"Perform basic image transformation and augmentation.","title":"ImageDataTransformer"},{"location":"temp/preprocessor/#attributes_1","text":"max_val : the maximum value of all data. mean : the mean value. std : the standard deviation. augment : whether to perform augmentation on data.","title":"Attributes"},{"location":"temp/preprocessor/#transform_train_2","text":"Transform the training data, perform random cropping data augmentation and basic random flip augmentation.","title":"transform_train"},{"location":"temp/preprocessor/#args_4","text":"data : Numpy array. The data to be transformed. batch_size : int batch_size. targets : the target of training set.","title":"Args"},{"location":"temp/preprocessor/#returns_3","text":"","title":"Returns"},{"location":"temp/preprocessor/#transform_test_2","text":"Transform the test data, perform normalization.","title":"transform_test"},{"location":"temp/preprocessor/#args_5","text":"data : Numpy array. The data to be transformed. batch_size : int batch_size. targets : the target of test set.","title":"Args"},{"location":"temp/preprocessor/#returns_4","text":"","title":"Returns"},{"location":"temp/preprocessor/#_transform","text":"Perform the actual transformation.","title":"_transform"},{"location":"temp/preprocessor/#args_6","text":"compose_list : a list of transforming operation. data : x. targets : y.","title":"Args"},{"location":"temp/preprocessor/#returns_5","text":"","title":"Returns"},{"location":"temp/preprocessor/#multitransformdataset","text":"A class incorporate all transform method into a torch.Dataset class.","title":"MultiTransformDataset"},{"location":"temp/preprocessor/#batchdataset","text":"A torch.Dataset class that can read data batch by batch.","title":"BatchDataset"},{"location":"temp/search/","text":"train Train the neural architecture. Searcher The base class to search for neural architectures. This class generate new architectures, call the trainer to train it, and update the optimizer. Attributes n_classes : Number of classes in the target classification task. input_shape : Arbitrary, although all dimensions in the input shaped must be fixed. Use the keyword argument input_shape (tuple of integers, does not include the batch axis) when using this layer as the first layer in a model. verbose : Verbosity mode. history : A list that stores the performance of model. Each element in it is a dictionary of 'model_id', 'loss', and 'metric_value'. neighbour_history : A list that stores the performance of neighbor of the best model. Each element in it is a dictionary of 'model_id', 'loss', and 'metric_value'. path : A string. The path to the directory for saving the searcher. metric : An instance of the Metric subclasses. loss : A function taking two parameters, the predictions and the ground truth. generators : A list of generators used to initialize the search. model_count : An integer. the total number of neural networks in the current searcher. descriptors : A dictionary of all the neural network architectures searched. trainer_args : A dictionary. The params for the constructor of ModelTrainer. default_model_len : An integer. Number of convolutional layers in the initial architecture. default_model_width : An integer. The number of filters in each layer in the initial architecture. training_queue : A list of the generated architectures to be trained. x_queue : A list of trained architectures not updated to the gpr. y_queue : A list of trained architecture performances not updated to the gpr. init Initialize the Searcher. Args n_output_node : An integer, the number of classes. input_shape : A tuple. e.g. (28, 28, 1). path : A string. The path to the directory to save the searcher. metric : An instance of the Metric subclasses. loss : A function taking two parameters, the predictions and the ground truth. generators : A list of generators used to initialize the search. verbose : A boolean. Whether to output the intermediate information to stdout. trainer_args : A dictionary. The params for the constructor of ModelTrainer. default_model_len : An integer. Number of convolutional layers in the initial architecture. default_model_width : An integer. The number of filters in each layer in the initial architecture. init_search Call the generators to generate the initial architectures for the search. search Run the search loop of training, generating and updating once. The function will run the training and generate in parallel. Then it will update the controller. The training is just pop out a graph from the training_queue and train it. The generate will call the self.generate function. The update will call the self.update function. Args train_data : An instance of DataLoader. test_data : An instance of Dataloader. timeout : An integer, time limit in seconds. generate Generate the next neural architecture. Args multiprocessing_queue : the Queue for multiprocessing return value. Returns list of 2-element tuples : generated_graph and other_info, generated_graph : An instance of Graph. other_info : Anything to be saved in the training queue together add_model Append the information of evaluated architecture to history. BayesianSearcher Class to search for neural architectures using Bayesian search strategy. Attribute: optimizer: An instance of BayesianOptimizer. t_min: A float. The minimum temperature during simulated annealing. generate Generate the next neural architecture. Args multiprocessing_queue : the Queue for multiprocessing return value. Returns list of 2-element tuples : generated_graph and other_info, generated_graph : An instance of Graph. other_info : Anything to be saved in the training queue together with the architecture. update Update the controller with evaluation result of a neural architecture. Args other_info : Anything. In our case it is the father ID in the search tree. model_id : An integer. graph : An instance of Graph. The trained neural architecture. metric_value : The final evaluated metric value.","title":"search"},{"location":"temp/search/#train","text":"Train the neural architecture.","title":"train"},{"location":"temp/search/#searcher","text":"The base class to search for neural architectures. This class generate new architectures, call the trainer to train it, and update the optimizer.","title":"Searcher"},{"location":"temp/search/#attributes","text":"n_classes : Number of classes in the target classification task. input_shape : Arbitrary, although all dimensions in the input shaped must be fixed. Use the keyword argument input_shape (tuple of integers, does not include the batch axis) when using this layer as the first layer in a model. verbose : Verbosity mode. history : A list that stores the performance of model. Each element in it is a dictionary of 'model_id', 'loss', and 'metric_value'. neighbour_history : A list that stores the performance of neighbor of the best model. Each element in it is a dictionary of 'model_id', 'loss', and 'metric_value'. path : A string. The path to the directory for saving the searcher. metric : An instance of the Metric subclasses. loss : A function taking two parameters, the predictions and the ground truth. generators : A list of generators used to initialize the search. model_count : An integer. the total number of neural networks in the current searcher. descriptors : A dictionary of all the neural network architectures searched. trainer_args : A dictionary. The params for the constructor of ModelTrainer. default_model_len : An integer. Number of convolutional layers in the initial architecture. default_model_width : An integer. The number of filters in each layer in the initial architecture. training_queue : A list of the generated architectures to be trained. x_queue : A list of trained architectures not updated to the gpr. y_queue : A list of trained architecture performances not updated to the gpr.","title":"Attributes"},{"location":"temp/search/#init","text":"Initialize the Searcher.","title":"init"},{"location":"temp/search/#args","text":"n_output_node : An integer, the number of classes. input_shape : A tuple. e.g. (28, 28, 1). path : A string. The path to the directory to save the searcher. metric : An instance of the Metric subclasses. loss : A function taking two parameters, the predictions and the ground truth. generators : A list of generators used to initialize the search. verbose : A boolean. Whether to output the intermediate information to stdout. trainer_args : A dictionary. The params for the constructor of ModelTrainer. default_model_len : An integer. Number of convolutional layers in the initial architecture. default_model_width : An integer. The number of filters in each layer in the initial architecture.","title":"Args"},{"location":"temp/search/#init_search","text":"Call the generators to generate the initial architectures for the search.","title":"init_search"},{"location":"temp/search/#search","text":"Run the search loop of training, generating and updating once. The function will run the training and generate in parallel. Then it will update the controller. The training is just pop out a graph from the training_queue and train it. The generate will call the self.generate function. The update will call the self.update function.","title":"search"},{"location":"temp/search/#args_1","text":"train_data : An instance of DataLoader. test_data : An instance of Dataloader. timeout : An integer, time limit in seconds.","title":"Args"},{"location":"temp/search/#generate","text":"Generate the next neural architecture.","title":"generate"},{"location":"temp/search/#args_2","text":"multiprocessing_queue : the Queue for multiprocessing return value.","title":"Args"},{"location":"temp/search/#returns","text":"list of 2-element tuples : generated_graph and other_info, generated_graph : An instance of Graph. other_info : Anything to be saved in the training queue together","title":"Returns"},{"location":"temp/search/#add_model","text":"Append the information of evaluated architecture to history.","title":"add_model"},{"location":"temp/search/#bayesiansearcher","text":"Class to search for neural architectures using Bayesian search strategy. Attribute: optimizer: An instance of BayesianOptimizer. t_min: A float. The minimum temperature during simulated annealing.","title":"BayesianSearcher"},{"location":"temp/search/#generate_1","text":"Generate the next neural architecture.","title":"generate"},{"location":"temp/search/#args_3","text":"multiprocessing_queue : the Queue for multiprocessing return value.","title":"Args"},{"location":"temp/search/#returns_1","text":"list of 2-element tuples : generated_graph and other_info, generated_graph : An instance of Graph. other_info : Anything to be saved in the training queue together with the architecture.","title":"Returns"},{"location":"temp/search/#update","text":"Update the controller with evaluation result of a neural architecture.","title":"update"},{"location":"temp/search/#args_4","text":"other_info : Anything. In our case it is the father ID in the search tree. model_id : An integer. graph : An instance of Graph. The trained neural architecture. metric_value : The final evaluated metric value.","title":"Args"},{"location":"temp/sentiment_analysis/","text":"","title":"Sentiment analysis"},{"location":"temp/supervised/","text":"Supervised The base class for all supervised task. Attributes verbose : A boolean value indicating the verbosity mode. init Initialize the instance. Args verbose : A boolean of whether the search process will be printed to stdout. fit Find the best neural architecture and train it. Based on the given dataset, the function will find the best neural architecture for it. The dataset is in numpy.ndarray format. So they training data should be passed through x_train , y_train . Args x : A numpy.ndarray instance containing the training data or the training data combined with the validation data. y : A numpy.ndarray instance containing the label of the training data. or the label of the training data combined with the validation label. time_limit : The time limit for the search in seconds. predict Return predict results for the testing data. Args x_test : An instance of numpy.ndarray containing the testing data. Returns evaluate Return the accuracy score between predict value and y_test . SearchSupervised The base class for all supervised task with architecture search. final_fit Final training after found the best architecture. Args x_train : A numpy.ndarray of training data. y_train : A numpy.ndarray of training targets. x_test : A numpy.ndarray of testing data. y_test : A numpy.ndarray of testing targets. trainer_args : A dictionary containing the parameters of the ModelTrainer constructor. retrain : A boolean of whether reinitialize the weights of the model. DeepTaskSupervised init Initialize the instance. The classifier will be loaded from the files in 'path' if parameter 'resume' is True. Otherwise it would create a new one. Args verbose : A boolean of whether the search process will be printed to stdout. path : A string. The path to a directory, where the intermediate results are saved. resume : A boolean. If True, the classifier will continue to previous work saved in path. Otherwise, the classifier will start a new search. searcher_args : A dictionary containing the parameters for the searcher's init function. search_type : A constant denoting the type of hyperparameter search algorithm that must be used. final_fit Final training after found the best architecture. Args x_train : A numpy.ndarray of training data. y_train : A numpy.ndarray of training targets. x_test : A numpy.ndarray of testing data. y_test : A numpy.ndarray of testing targets. trainer_args : A dictionary containing the parameters of the ModelTrainer constructor. retrain : A boolean of whether reinitialize the weights of the model. export_keras_model Exports the best Keras model to the given filename. predict Return predict results for the testing data. Args x_test : An instance of numpy.ndarray containing the testing data. Returns evaluate Return the accuracy score between predict value and y_test . SingleModelSupervised The base class for all supervised task without architecture search. init Initialize the instance. Args verbose : A boolean of whether the search process will be printed to stdout. path : A string. The path to a directory, where the intermediate results are saved. predict Return predict results for the testing data. Args x_test : An instance of numpy.ndarray containing the testing data. Returns evaluate Return the accuracy score between predict value and y_test . save Save the model as keras format. Args model_path : the path to save model. PortableDeepSupervised init Initialize the instance. Args graph : The graph form of the learned model. y_encoder : The encoder of the label. See example as OneHotEncoder data_transformer : A transformer class to process the data. See example as ImageDataTransformer. verbose : A boolean of whether the search process will be printed to stdout. path : A string. The path to a directory, where the intermediate results are saved. fit Trains the model on the dataset given. Args x : A numpy.ndarray instance containing the training data or the training data combined with the validation data. y : A numpy.ndarray instance containing the label of the training data. or the label of the training data combined with the validation label. trainer_args : A dictionary containing the parameters of the ModelTrainer constructor. retrain : A boolean of whether reinitialize the weights of the model.","title":"supervised"},{"location":"temp/supervised/#supervised","text":"The base class for all supervised task.","title":"Supervised"},{"location":"temp/supervised/#attributes","text":"verbose : A boolean value indicating the verbosity mode.","title":"Attributes"},{"location":"temp/supervised/#init","text":"Initialize the instance.","title":"init"},{"location":"temp/supervised/#args","text":"verbose : A boolean of whether the search process will be printed to stdout.","title":"Args"},{"location":"temp/supervised/#fit","text":"Find the best neural architecture and train it. Based on the given dataset, the function will find the best neural architecture for it. The dataset is in numpy.ndarray format. So they training data should be passed through x_train , y_train .","title":"fit"},{"location":"temp/supervised/#args_1","text":"x : A numpy.ndarray instance containing the training data or the training data combined with the validation data. y : A numpy.ndarray instance containing the label of the training data. or the label of the training data combined with the validation label. time_limit : The time limit for the search in seconds.","title":"Args"},{"location":"temp/supervised/#predict","text":"Return predict results for the testing data.","title":"predict"},{"location":"temp/supervised/#args_2","text":"x_test : An instance of numpy.ndarray containing the testing data.","title":"Args"},{"location":"temp/supervised/#returns","text":"","title":"Returns"},{"location":"temp/supervised/#evaluate","text":"Return the accuracy score between predict value and y_test .","title":"evaluate"},{"location":"temp/supervised/#searchsupervised","text":"The base class for all supervised task with architecture search.","title":"SearchSupervised"},{"location":"temp/supervised/#final_fit","text":"Final training after found the best architecture.","title":"final_fit"},{"location":"temp/supervised/#args_3","text":"x_train : A numpy.ndarray of training data. y_train : A numpy.ndarray of training targets. x_test : A numpy.ndarray of testing data. y_test : A numpy.ndarray of testing targets. trainer_args : A dictionary containing the parameters of the ModelTrainer constructor. retrain : A boolean of whether reinitialize the weights of the model.","title":"Args"},{"location":"temp/supervised/#deeptasksupervised","text":"","title":"DeepTaskSupervised"},{"location":"temp/supervised/#init_1","text":"Initialize the instance. The classifier will be loaded from the files in 'path' if parameter 'resume' is True. Otherwise it would create a new one.","title":"init"},{"location":"temp/supervised/#args_4","text":"verbose : A boolean of whether the search process will be printed to stdout. path : A string. The path to a directory, where the intermediate results are saved. resume : A boolean. If True, the classifier will continue to previous work saved in path. Otherwise, the classifier will start a new search. searcher_args : A dictionary containing the parameters for the searcher's init function. search_type : A constant denoting the type of hyperparameter search algorithm that must be used.","title":"Args"},{"location":"temp/supervised/#final_fit_1","text":"Final training after found the best architecture.","title":"final_fit"},{"location":"temp/supervised/#args_5","text":"x_train : A numpy.ndarray of training data. y_train : A numpy.ndarray of training targets. x_test : A numpy.ndarray of testing data. y_test : A numpy.ndarray of testing targets. trainer_args : A dictionary containing the parameters of the ModelTrainer constructor. retrain : A boolean of whether reinitialize the weights of the model.","title":"Args"},{"location":"temp/supervised/#export_keras_model","text":"Exports the best Keras model to the given filename.","title":"export_keras_model"},{"location":"temp/supervised/#predict_1","text":"Return predict results for the testing data.","title":"predict"},{"location":"temp/supervised/#args_6","text":"x_test : An instance of numpy.ndarray containing the testing data.","title":"Args"},{"location":"temp/supervised/#returns_1","text":"","title":"Returns"},{"location":"temp/supervised/#evaluate_1","text":"Return the accuracy score between predict value and y_test .","title":"evaluate"},{"location":"temp/supervised/#singlemodelsupervised","text":"The base class for all supervised task without architecture search.","title":"SingleModelSupervised"},{"location":"temp/supervised/#init_2","text":"Initialize the instance.","title":"init"},{"location":"temp/supervised/#args_7","text":"verbose : A boolean of whether the search process will be printed to stdout. path : A string. The path to a directory, where the intermediate results are saved.","title":"Args"},{"location":"temp/supervised/#predict_2","text":"Return predict results for the testing data.","title":"predict"},{"location":"temp/supervised/#args_8","text":"x_test : An instance of numpy.ndarray containing the testing data.","title":"Args"},{"location":"temp/supervised/#returns_2","text":"","title":"Returns"},{"location":"temp/supervised/#evaluate_2","text":"Return the accuracy score between predict value and y_test .","title":"evaluate"},{"location":"temp/supervised/#save","text":"Save the model as keras format.","title":"save"},{"location":"temp/supervised/#args_9","text":"model_path : the path to save model.","title":"Args"},{"location":"temp/supervised/#portabledeepsupervised","text":"","title":"PortableDeepSupervised"},{"location":"temp/supervised/#init_3","text":"Initialize the instance.","title":"init"},{"location":"temp/supervised/#args_10","text":"graph : The graph form of the learned model. y_encoder : The encoder of the label. See example as OneHotEncoder data_transformer : A transformer class to process the data. See example as ImageDataTransformer. verbose : A boolean of whether the search process will be printed to stdout. path : A string. The path to a directory, where the intermediate results are saved.","title":"Args"},{"location":"temp/supervised/#fit_1","text":"Trains the model on the dataset given.","title":"fit"},{"location":"temp/supervised/#args_11","text":"x : A numpy.ndarray instance containing the training data or the training data combined with the validation data. y : A numpy.ndarray instance containing the label of the training data. or the label of the training data combined with the validation label. trainer_args : A dictionary containing the parameters of the ModelTrainer constructor. retrain : A boolean of whether reinitialize the weights of the model.","title":"Args"},{"location":"temp/symbols/","text":"","title":"Symbols"},{"location":"temp/tabular_preprocessor/","text":"TabularPreprocessor init Initialization function for tabular preprocessor. fit This function should train the model parameters. Args raw_x : a numpy.ndarray instance containing the training data. y : training label vector. time_limit : remaining time budget. data_info : meta-features of the dataset, which is an numpy.ndarray describing the feature type of each column in raw_x. The feature type include encode This function should train the model parameters. Args raw_x : a numpy.ndarray instance containing the training/testing data. time_limit : remaining time budget. inputs X and y are numpy arrays. extract_data_info This function extracts the data info automatically based on the type of each feature in raw_x. Args raw_x : a numpy.ndarray instance containing the training data.","title":"Tabular preprocessor"},{"location":"temp/tabular_preprocessor/#tabularpreprocessor","text":"","title":"TabularPreprocessor"},{"location":"temp/tabular_preprocessor/#init","text":"Initialization function for tabular preprocessor.","title":"init"},{"location":"temp/tabular_preprocessor/#fit","text":"This function should train the model parameters.","title":"fit"},{"location":"temp/tabular_preprocessor/#args","text":"raw_x : a numpy.ndarray instance containing the training data. y : training label vector. time_limit : remaining time budget. data_info : meta-features of the dataset, which is an numpy.ndarray describing the feature type of each column in raw_x. The feature type include","title":"Args"},{"location":"temp/tabular_preprocessor/#encode","text":"This function should train the model parameters.","title":"encode"},{"location":"temp/tabular_preprocessor/#args_1","text":"raw_x : a numpy.ndarray instance containing the training/testing data. time_limit : remaining time budget. inputs X and y are numpy arrays.","title":"Args"},{"location":"temp/tabular_preprocessor/#extract_data_info","text":"This function extracts the data info automatically based on the type of each feature in raw_x.","title":"extract_data_info"},{"location":"temp/tabular_preprocessor/#args_2","text":"raw_x : a numpy.ndarray instance containing the training data.","title":"Args"},{"location":"temp/tabular_supervised/","text":"TabularSupervised init Initialization function for tabular supervised learner. fit This function should train the model parameters. Args x : A numpy.ndarray instance containing the training data. y : training label vector. time_limit : remaining time budget. data_info : meta-features of the dataset, which is an numpy.ndarray describing the feature type of each column in raw_x. The feature type include predict This function should provide predictions of labels on (test) data. The function predict eventually casdn return probabilities or continuous values. TabularRegressor TabularRegressor class. It is used for tabular data regression with lightgbm regressor. TabularClassifier TabularClassifier class. It is used for tabular data classification with lightgbm classifier.","title":"Tabular supervised"},{"location":"temp/tabular_supervised/#tabularsupervised","text":"","title":"TabularSupervised"},{"location":"temp/tabular_supervised/#init","text":"Initialization function for tabular supervised learner.","title":"init"},{"location":"temp/tabular_supervised/#fit","text":"This function should train the model parameters.","title":"fit"},{"location":"temp/tabular_supervised/#args","text":"x : A numpy.ndarray instance containing the training data. y : training label vector. time_limit : remaining time budget. data_info : meta-features of the dataset, which is an numpy.ndarray describing the feature type of each column in raw_x. The feature type include","title":"Args"},{"location":"temp/tabular_supervised/#predict","text":"This function should provide predictions of labels on (test) data. The function predict eventually casdn return probabilities or continuous values.","title":"predict"},{"location":"temp/tabular_supervised/#tabularregressor","text":"TabularRegressor class. It is used for tabular data regression with lightgbm regressor.","title":"TabularRegressor"},{"location":"temp/tabular_supervised/#tabularclassifier","text":"TabularClassifier class. It is used for tabular data classification with lightgbm classifier.","title":"TabularClassifier"},{"location":"temp/text/","text":"text_to_sequence Converts a string of text to a sequence of IDs corresponding to the symbols in the text. The text can optionally have ARPAbet sequences enclosed in curly braces embedded in it. For example, \"Turn left on {HH AW1 S S T AH0 N} Street.\" Args text : string to convert to a sequence cleaner_names : names of the cleaner functions to run the text through Returns","title":"Text"},{"location":"temp/text/#text_to_sequence","text":"Converts a string of text to a sequence of IDs corresponding to the symbols in the text. The text can optionally have ARPAbet sequences enclosed in curly braces embedded in it. For example, \"Turn left on {HH AW1 S S T AH0 N} Street.\"","title":"text_to_sequence"},{"location":"temp/text/#args","text":"text : string to convert to a sequence cleaner_names : names of the cleaner functions to run the text through","title":"Args"},{"location":"temp/text/#returns","text":"","title":"Returns"},{"location":"temp/text_preprocessor/","text":"download_pre_train Download pre train file from link in constant.py. Args file_path : String, contains download file path + file name. extract_path : String extract path name. clean_str Tokenization/string cleaning for all string. Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py tokenlize_text Tokenlize text. Vectorize a text corpus by transform each text in texts to a sequence of integers. Args max_num_words : Int, max number of words in the dictionary. max_seq_length : Int, the length of each text sequence, padding if shorter, trim is longer. x_train : List contains text data. Returns x_train : Tokenlized input data. word_index : Dictionary contains word with tokenlized index. read_embedding_index Read pre train file convert to embedding vector. Read the pre trained file into a dictionary where key is the word and value is embedding vector. Args extract_path : String contains pre trained file path. Returns embedding_index : Dictionary contains word with pre trained index. load_pretrain Load the pretrain file into embedding weights. This method will first generate the embedding index and then generate embedding matrix according to the word_index. Args path : String, path to store the pretrain files. word_index : Dictionary contains word with tokenlized index. Returns embedding_matrix : Numpy array as the pretrain model embedding layer weights. processing Processing string array with pretrained vectors. convert an n dimension string array into n * k * m dimension float numpy array. Each k * m array represents a string. k is the input_length which means an upper bound of the string length, for string shorter than k will be pad and longer string will be cropped. m is defined by the pretrained file. Args path : String, path where the pre trained files stored. word_index : Dictionary, contains word with tokenlized index. input_length : Int, an upper bound of the string length. x_train : String array. Returns x_train : Numpy array as processed x_train. text_preprocess This is the text preprocess main method. It takes an raw string, clean it and processing it into tokenlized numpy array.","title":"Text preprocessor"},{"location":"temp/text_preprocessor/#download_pre_train","text":"Download pre train file from link in constant.py.","title":"download_pre_train"},{"location":"temp/text_preprocessor/#args","text":"file_path : String, contains download file path + file name. extract_path : String extract path name.","title":"Args"},{"location":"temp/text_preprocessor/#clean_str","text":"Tokenization/string cleaning for all string. Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py","title":"clean_str"},{"location":"temp/text_preprocessor/#tokenlize_text","text":"Tokenlize text. Vectorize a text corpus by transform each text in texts to a sequence of integers.","title":"tokenlize_text"},{"location":"temp/text_preprocessor/#args_1","text":"max_num_words : Int, max number of words in the dictionary. max_seq_length : Int, the length of each text sequence, padding if shorter, trim is longer. x_train : List contains text data.","title":"Args"},{"location":"temp/text_preprocessor/#returns","text":"x_train : Tokenlized input data. word_index : Dictionary contains word with tokenlized index.","title":"Returns"},{"location":"temp/text_preprocessor/#read_embedding_index","text":"Read pre train file convert to embedding vector. Read the pre trained file into a dictionary where key is the word and value is embedding vector.","title":"read_embedding_index"},{"location":"temp/text_preprocessor/#args_2","text":"extract_path : String contains pre trained file path.","title":"Args"},{"location":"temp/text_preprocessor/#returns_1","text":"embedding_index : Dictionary contains word with pre trained index.","title":"Returns"},{"location":"temp/text_preprocessor/#load_pretrain","text":"Load the pretrain file into embedding weights. This method will first generate the embedding index and then generate embedding matrix according to the word_index.","title":"load_pretrain"},{"location":"temp/text_preprocessor/#args_3","text":"path : String, path to store the pretrain files. word_index : Dictionary contains word with tokenlized index.","title":"Args"},{"location":"temp/text_preprocessor/#returns_2","text":"embedding_matrix : Numpy array as the pretrain model embedding layer weights.","title":"Returns"},{"location":"temp/text_preprocessor/#processing","text":"Processing string array with pretrained vectors. convert an n dimension string array into n * k * m dimension float numpy array. Each k * m array represents a string. k is the input_length which means an upper bound of the string length, for string shorter than k will be pad and longer string will be cropped. m is defined by the pretrained file.","title":"processing"},{"location":"temp/text_preprocessor/#args_4","text":"path : String, path where the pre trained files stored. word_index : Dictionary, contains word with tokenlized index. input_length : Int, an upper bound of the string length. x_train : String array.","title":"Args"},{"location":"temp/text_preprocessor/#returns_3","text":"x_train : Numpy array as processed x_train.","title":"Returns"},{"location":"temp/text_preprocessor/#text_preprocess","text":"This is the text preprocess main method. It takes an raw string, clean it and processing it into tokenlized numpy array.","title":"text_preprocess"},{"location":"temp/text_supervised/","text":"TextSupervised TextClassifier class. Attributes cnn : CNN module from net_module.py. path : A path to the directory to save the classifier as well as intermediate results. y_encoder : Label encoder, used in transform_y or inverse_transform_y for encode the label. For example, if one hot encoder needed, y_encoder can be OneHotEncoder. data_transformer : A transformer class to process the data. See example as ImageDataTransformer. verbose : A boolean value indicating the verbosity mode which determines whether the search process will be printed to stdout. init Initialize the instance. The classifier will be loaded from the files in 'path' if parameter 'resume' is True. Otherwise it would create a new one. Args verbose : A boolean of whether the search process will be printed to stdout. path : A string. The path to a directory, where the intermediate results are saved. resume : A boolean. If True, the classifier will continue to previous work saved in path. Otherwise, the classifier will start a new search. searcher_args : A dictionary containing the parameters for the searcher's init function. fit Find the best neural architecture and train it. Based on the given dataset, the function will find the best neural architecture for it. The dataset is in numpy.ndarray format. So they training data should be passed through x_train , y_train . Args x : A numpy.ndarray instance containing the training data. y : A numpy.ndarray instance containing the label of the training data. time_limit : The time limit for the search in seconds. TextRegressor TextRegressor class. It is used for text regression. It searches convolutional neural network architectures for the best configuration for the text dataset.","title":"Text supervised"},{"location":"temp/text_supervised/#textsupervised","text":"TextClassifier class.","title":"TextSupervised"},{"location":"temp/text_supervised/#attributes","text":"cnn : CNN module from net_module.py. path : A path to the directory to save the classifier as well as intermediate results. y_encoder : Label encoder, used in transform_y or inverse_transform_y for encode the label. For example, if one hot encoder needed, y_encoder can be OneHotEncoder. data_transformer : A transformer class to process the data. See example as ImageDataTransformer. verbose : A boolean value indicating the verbosity mode which determines whether the search process will be printed to stdout.","title":"Attributes"},{"location":"temp/text_supervised/#init","text":"Initialize the instance. The classifier will be loaded from the files in 'path' if parameter 'resume' is True. Otherwise it would create a new one.","title":"init"},{"location":"temp/text_supervised/#args","text":"verbose : A boolean of whether the search process will be printed to stdout. path : A string. The path to a directory, where the intermediate results are saved. resume : A boolean. If True, the classifier will continue to previous work saved in path. Otherwise, the classifier will start a new search. searcher_args : A dictionary containing the parameters for the searcher's init function.","title":"Args"},{"location":"temp/text_supervised/#fit","text":"Find the best neural architecture and train it. Based on the given dataset, the function will find the best neural architecture for it. The dataset is in numpy.ndarray format. So they training data should be passed through x_train , y_train .","title":"fit"},{"location":"temp/text_supervised/#args_1","text":"x : A numpy.ndarray instance containing the training data. y : A numpy.ndarray instance containing the label of the training data. time_limit : The time limit for the search in seconds.","title":"Args"},{"location":"temp/text_supervised/#textregressor","text":"TextRegressor class. It is used for text regression. It searches convolutional neural network architectures for the best configuration for the text dataset.","title":"TextRegressor"},{"location":"temp/tokenization/","text":"load_vocab Loads a vocabulary file into a dictionary. whitespace_tokenize Runs basic whitespace cleaning and splitting on a peice of text. _is_whitespace Checks whether chars is a whitespace character. _is_control Checks whether chars is a control character. _is_punctuation Checks whether chars is a punctuation character. BertTokenizer Runs end-to-end tokenization: punctuation splitting + wordpiece convert_tokens_to_ids Converts a sequence of tokens into ids using the vocab. convert_ids_to_tokens Converts a sequence of ids in wordpiece tokens using the vocab. from_pretrained Instantiate a PreTrainedBertModel from a pre-trained model file. Download and cache the pre-trained model file if needed. BasicTokenizer Runs basic tokenization (punctuation splitting, lower casing, etc.). init Constructs a BasicTokenizer. Args do_lower_case : Whether to lower case the input. tokenize Tokenizes a piece of text. _run_strip_accents Strips accents from a piece of text. _run_split_on_punc Splits punctuation on a piece of text. _tokenize_chinese_chars Adds whitespace around any CJK character. _is_chinese_char Checks whether CP is the codepoint of a CJK character. _clean_text Performs invalid character removal and whitespace cleanup on text. WordpieceTokenizer Runs WordPiece tokenization. tokenize Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform tokenization using the given vocabulary. For example: input = \"unaffable\" output = [\"un\", \"##aff\", \"##able\"] Args text : A single token or whitespace separated tokens. This should have already been passed through `BasicTokenizer. Returns","title":"Tokenization"},{"location":"temp/tokenization/#load_vocab","text":"Loads a vocabulary file into a dictionary.","title":"load_vocab"},{"location":"temp/tokenization/#whitespace_tokenize","text":"Runs basic whitespace cleaning and splitting on a peice of text.","title":"whitespace_tokenize"},{"location":"temp/tokenization/#_is_whitespace","text":"Checks whether chars is a whitespace character.","title":"_is_whitespace"},{"location":"temp/tokenization/#_is_control","text":"Checks whether chars is a control character.","title":"_is_control"},{"location":"temp/tokenization/#_is_punctuation","text":"Checks whether chars is a punctuation character.","title":"_is_punctuation"},{"location":"temp/tokenization/#berttokenizer","text":"Runs end-to-end tokenization: punctuation splitting + wordpiece","title":"BertTokenizer"},{"location":"temp/tokenization/#convert_tokens_to_ids","text":"Converts a sequence of tokens into ids using the vocab.","title":"convert_tokens_to_ids"},{"location":"temp/tokenization/#convert_ids_to_tokens","text":"Converts a sequence of ids in wordpiece tokens using the vocab.","title":"convert_ids_to_tokens"},{"location":"temp/tokenization/#from_pretrained","text":"Instantiate a PreTrainedBertModel from a pre-trained model file. Download and cache the pre-trained model file if needed.","title":"from_pretrained"},{"location":"temp/tokenization/#basictokenizer","text":"Runs basic tokenization (punctuation splitting, lower casing, etc.).","title":"BasicTokenizer"},{"location":"temp/tokenization/#init","text":"Constructs a BasicTokenizer.","title":"init"},{"location":"temp/tokenization/#args","text":"do_lower_case : Whether to lower case the input.","title":"Args"},{"location":"temp/tokenization/#tokenize","text":"Tokenizes a piece of text.","title":"tokenize"},{"location":"temp/tokenization/#_run_strip_accents","text":"Strips accents from a piece of text.","title":"_run_strip_accents"},{"location":"temp/tokenization/#_run_split_on_punc","text":"Splits punctuation on a piece of text.","title":"_run_split_on_punc"},{"location":"temp/tokenization/#_tokenize_chinese_chars","text":"Adds whitespace around any CJK character.","title":"_tokenize_chinese_chars"},{"location":"temp/tokenization/#_is_chinese_char","text":"Checks whether CP is the codepoint of a CJK character.","title":"_is_chinese_char"},{"location":"temp/tokenization/#_clean_text","text":"Performs invalid character removal and whitespace cleanup on text.","title":"_clean_text"},{"location":"temp/tokenization/#wordpiecetokenizer","text":"Runs WordPiece tokenization.","title":"WordpieceTokenizer"},{"location":"temp/tokenization/#tokenize_1","text":"Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform tokenization using the given vocabulary. For example: input = \"unaffable\" output = [\"un\", \"##aff\", \"##able\"]","title":"tokenize"},{"location":"temp/tokenization/#args_1","text":"text : A single token or whitespace separated tokens. This should have already been passed through `BasicTokenizer.","title":"Args"},{"location":"temp/tokenization/#returns","text":"","title":"Returns"},{"location":"temp/unsupervised/","text":"Unsupervised The base class for all unsupervised task Attributes verbose : A boolean value indicating the verbosity mode. init Args: verbose: A boolean of whether the search process will be printed to stdout. fit Args: x_train: A numpy.ndarray instance containing the training data. generate Args: A numpy.ndarray or torch.tensor input fed into the model to generate the output","title":"Unsupervised"},{"location":"temp/unsupervised/#unsupervised","text":"The base class for all unsupervised task","title":"Unsupervised"},{"location":"temp/unsupervised/#attributes","text":"verbose : A boolean value indicating the verbosity mode.","title":"Attributes"},{"location":"temp/unsupervised/#init","text":"Args: verbose: A boolean of whether the search process will be printed to stdout.","title":"init"},{"location":"temp/unsupervised/#fit","text":"Args: x_train: A numpy.ndarray instance containing the training data.","title":"fit"},{"location":"temp/unsupervised/#generate","text":"Args: A numpy.ndarray or torch.tensor input fed into the model to generate the output","title":"generate"},{"location":"temp/utils/","text":"ensure_dir Create directory if it does not exist. ensure_file_dir Create path if it does not exist. has_file Check if the given path exists. pickle_from_file Load the pickle file from the provided path and returns the object. pickle_to_file Save the pickle file to the specified path. get_device If CUDA is available, use CUDA device, else use CPU device. Returns: string device name rand_temp_folder_generator Create and return a temporary directory with the path name '/temp_dir_name/autokeras' (E:g:- /tmp/autokeras). download_file Download the file specified in file_link and saves it in file_path . download_file_with_extract Download the file specified in file_link , save to file_path and extract to the directory extract_path . get_confirm_token If there is a warning when download is requested, return the token value. Otherwise, return None. save_response_content Save the HTTP GET response of the download request in the destination. download_file_from_google_drive Download a file with the specified Google Drive Id and save it to the destination. verbose_print Print information about the operation performed on father model to obtain current model and father's id. validate_xy Validate x_train 's type and the shape of x_train , y_train . read_csv_file Read the csv file and returns two separate list containing file names and their labels. Args csv_file_path : Path to the CSV file. Returns file_names : List containing files names. file_label : List containing their respective labels. read_image Read the image contained in the provided path image_path . compute_image_resize_params Compute median dimension of all images in data. It used to resize the images later. Number of channels do not change from the original data. Args data : 1-D, 2-D or 3-D images. The Images are expected to have channel last configuration. Returns resize_image_data Resize images to given dimension. Args data : 1-D, 2-D or 3-D images. The Images are expected to have channel last configuration. resize_shape : Image resize dimension. Returns data : Reshaped data. get_system Get the current system environment. If the current system is not supported, raise an exception. Returns download_file_from_google_drive Downloads a shared file from google drive into a given folder. Optionally unzips it. Refact from: https://github.com/ndrplz/google-drive-downloader/blob/master/google_drive_downloader/google_drive_downloader.py Parameters ---------- file_id: str the file identifier. You can obtain it from the sharable link. dest_path: str the destination where to save the downloaded file. Must be a path (for example: './downloaded_file.txt') overwrite: bool optional, if True forces re-download and overwrite. unzip: bool optional, if True unzips a file. If the file is not a zip file, ignores it. Returns","title":"utils"},{"location":"temp/utils/#ensure_dir","text":"Create directory if it does not exist.","title":"ensure_dir"},{"location":"temp/utils/#ensure_file_dir","text":"Create path if it does not exist.","title":"ensure_file_dir"},{"location":"temp/utils/#has_file","text":"Check if the given path exists.","title":"has_file"},{"location":"temp/utils/#pickle_from_file","text":"Load the pickle file from the provided path and returns the object.","title":"pickle_from_file"},{"location":"temp/utils/#pickle_to_file","text":"Save the pickle file to the specified path.","title":"pickle_to_file"},{"location":"temp/utils/#get_device","text":"If CUDA is available, use CUDA device, else use CPU device. Returns: string device name","title":"get_device"},{"location":"temp/utils/#rand_temp_folder_generator","text":"Create and return a temporary directory with the path name '/temp_dir_name/autokeras' (E:g:- /tmp/autokeras).","title":"rand_temp_folder_generator"},{"location":"temp/utils/#download_file","text":"Download the file specified in file_link and saves it in file_path .","title":"download_file"},{"location":"temp/utils/#download_file_with_extract","text":"Download the file specified in file_link , save to file_path and extract to the directory extract_path .","title":"download_file_with_extract"},{"location":"temp/utils/#get_confirm_token","text":"If there is a warning when download is requested, return the token value. Otherwise, return None.","title":"get_confirm_token"},{"location":"temp/utils/#save_response_content","text":"Save the HTTP GET response of the download request in the destination.","title":"save_response_content"},{"location":"temp/utils/#download_file_from_google_drive","text":"Download a file with the specified Google Drive Id and save it to the destination.","title":"download_file_from_google_drive"},{"location":"temp/utils/#verbose_print","text":"Print information about the operation performed on father model to obtain current model and father's id.","title":"verbose_print"},{"location":"temp/utils/#validate_xy","text":"Validate x_train 's type and the shape of x_train , y_train .","title":"validate_xy"},{"location":"temp/utils/#read_csv_file","text":"Read the csv file and returns two separate list containing file names and their labels.","title":"read_csv_file"},{"location":"temp/utils/#args","text":"csv_file_path : Path to the CSV file.","title":"Args"},{"location":"temp/utils/#returns","text":"file_names : List containing files names. file_label : List containing their respective labels.","title":"Returns"},{"location":"temp/utils/#read_image","text":"Read the image contained in the provided path image_path .","title":"read_image"},{"location":"temp/utils/#compute_image_resize_params","text":"Compute median dimension of all images in data. It used to resize the images later. Number of channels do not change from the original data.","title":"compute_image_resize_params"},{"location":"temp/utils/#args_1","text":"data : 1-D, 2-D or 3-D images. The Images are expected to have channel last configuration.","title":"Args"},{"location":"temp/utils/#returns_1","text":"","title":"Returns"},{"location":"temp/utils/#resize_image_data","text":"Resize images to given dimension.","title":"resize_image_data"},{"location":"temp/utils/#args_2","text":"data : 1-D, 2-D or 3-D images. The Images are expected to have channel last configuration. resize_shape : Image resize dimension.","title":"Args"},{"location":"temp/utils/#returns_2","text":"data : Reshaped data.","title":"Returns"},{"location":"temp/utils/#get_system","text":"Get the current system environment. If the current system is not supported, raise an exception.","title":"get_system"},{"location":"temp/utils/#returns_3","text":"","title":"Returns"},{"location":"temp/utils/#download_file_from_google_drive_1","text":"Downloads a shared file from google drive into a given folder. Optionally unzips it. Refact from: https://github.com/ndrplz/google-drive-downloader/blob/master/google_drive_downloader/google_drive_downloader.py Parameters ---------- file_id: str the file identifier. You can obtain it from the sharable link. dest_path: str the destination where to save the downloaded file. Must be a path (for example: './downloaded_file.txt') overwrite: bool optional, if True forces re-download and overwrite. unzip: bool optional, if True unzips a file. If the file is not a zip file, ignores it.","title":"download_file_from_google_drive"},{"location":"temp/utils/#returns_4","text":"","title":"Returns"},{"location":"temp/version/","text":"","title":"Version"},{"location":"temp/voice_generator/","text":"inv_preemphasis Inverse operation of pre-emphasis Args x (1d-array) : Input signal. coef (float) : Pre-emphasis coefficient. Returns array : Output filtered signal. also : ****: func inv_spectrogram Converts spectrogram to waveform using librosa VoiceGenerator tts Convert text to speech waveform given a deepvoice3 model. Args text (str) : Input text to be synthesized p (float) : Replace word to pronounciation if p > 0. Default is 0.","title":"Voice generator"},{"location":"temp/voice_generator/#inv_preemphasis","text":"Inverse operation of pre-emphasis","title":"inv_preemphasis"},{"location":"temp/voice_generator/#args","text":"x (1d-array) : Input signal. coef (float) : Pre-emphasis coefficient.","title":"Args"},{"location":"temp/voice_generator/#returns","text":"array : Output filtered signal. also : ****: func","title":"Returns"},{"location":"temp/voice_generator/#inv_spectrogram","text":"Converts spectrogram to waveform using librosa","title":"inv_spectrogram"},{"location":"temp/voice_generator/#voicegenerator","text":"","title":"VoiceGenerator"},{"location":"temp/voice_generator/#tts","text":"Convert text to speech waveform given a deepvoice3 model.","title":"tts"},{"location":"temp/voice_generator/#args_1","text":"text (str) : Input text to be synthesized p (float) : Replace word to pronounciation if p > 0. Default is 0.","title":"Args"}]}